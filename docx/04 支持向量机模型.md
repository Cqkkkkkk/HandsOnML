## 支持向量机模型

​	支持向量机（*Support Vector Machine*）是一种强大而全能的机器学习模型，其适用范围包括执行线性或者非线性的分类、回归，以及越界检测。支持向量机算法特别适用于**对中小数据规模、特征极为复杂的数据**的分类。例如，数据集只包含100个样本，但每个样本可能有上百个参数。

### 线性SVM分类器

​	例如，我们有下图中所示的两类数据。我们之前使用线性分类器对这些数据分类，本质上是使用了一条直线将数据划分为两个区间，每个区间代表一个类别。例如左图中，绿色虚线代表的分类方法显然是十分不合理的；同时，紫色和黄色的实线进行的分类也不是特别理想，因为他们的决策边界（也就是这条分类直线）和某些样本的实例实在是太接近了，以至于在出现新的实例时，很可能其会因为过于逼近的决策边界而被误分类到另一个类别中。

​	而SVM分类器将会得到如右图所示的实线对应的决策边界。该决策边界不仅把两个类别的实例分开了，同时还和离它最近的实例尽可能地远，这样就使得新出现的实例被误分类的可能性远比之前低。在这里，这种方法也被称为大间距分类器（*large margin classification*）。

![image-20200502202546228](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200502202546228.png)

​	注意对于线性SVM方法，添加更多的远离边界的实例是不会改变决策边界的位置的。决策边界由离边界最近的那些实例决定（或者说“支持”），这些实例被称为*支持向量*。

​	注意，**支持向量机对数据特征的量级非常敏感**，如果我们的数据有两个特征，其取值范围分别为$(0,100)$与$(0,1)$，那么如果不把这两个数据的量级(Scale)统一到统一标准，SVM将会得到非常不理想的结果，因为这时候**距离之间的权重**发生了改变，那些数据本身量级比较大的特征将会对距离产生非常大的影响，反之那些本身数据比较小的特征起到的影响则微乎其微，尽管这两个特征可能同样重要。

![image-20200502202558388](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200502202558388.png)

#### 软间隔分类

​	如果我们严格要求SVM得出的决策边界将属于两个类别的实例分开，那么该方法称为**硬间隔分类**（*Hard Margin Classification*）。该方法有两个主要的缺陷，其一是它只有在数据本身是线性可分的情况下才能工作，其次它对某些“特例”非常敏感。

![image-20200502202618847](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200502202618847.png)

​	想避免这些缺陷我们就要使用更为灵活的模型，如**软间隔分类**（*Soft Margin Classification*）。如果我们把决策边界，及其附近的空白区域比作一条大街，用这条大街来分割区域，那么我们的目的在于：*使得这个决策边界离两侧样本都尽可能的远（这条大街尽可能的宽），同时又限制边界冲突（例如，那些处于大街中央，甚至处于大街对侧的实例）的存在。*

​	在sklearn的的SVM类中，我们可以通过设置超参数$C$来控制上述平衡。一个较小的参数C将会导致更宽的街道，同时也会有更多的边界冲突。一个较大的C则会使得模型出发更少的边界冲突，但是相应的街道的宽度也会随之下降。一般来说，*选择较小的C的支持向量机模型对分类任务的效果更好*，这是因为大多数情况下，边界冲突也是发生在决策边界正确的一侧，不会真正导致决策错误。特别地，*如果发现模型出现了过拟合现象，可以通过减小C来减轻过拟合现象*。 

![image-20200502202629000](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200502202629000.png)

​	下面的`sklearn`代码载入了数据集，对数据的量级进行了规范化，并且基于此训练了一个线性SVM模型（使用`LinearSVC`类并设置C=0.1）

```python
import numpy as np 
from sklearn import datasets 
from sklearn.pipeline import Pipeline 
from sklearn.preprocessing import StandardScaler 
from sklearn.svm import LinearSVC
iris = datasets.load_iris() 
X = iris["data"][:, (2, 3)] # petal length, petal width
y = (iris["target"] == 2).astype(np.float64) # Iris-Virginica
svm_clf = Pipeline(( 
    ("scaler", StandardScaler()), 
    ("linear_svc", LinearSVC(C=1, loss="hinge")),
)) 
svm_clf.fit(X_scaled, y)
# 接着就可以进行预测了
```

>   不同于Logistic回归分类器，SVM分类器不会输出每个类别的概率。

另外，除了直接使用`LinearSVC`类之外，我们还可以使用`SVC(kernel='linear', C=1)`来完成上述操作，不过这样会比之前慢很多，特别是在训练集比较大的时候，故而我们不推荐这么干。

或者我们可以使用`SGDClassifier(loss='hinge', alpha=1/(m*C))`来进行常规的随机梯度下降方法来训练一个线性的SVM模型。这种方法同样没有`LineraSVC`快，但是它在处理巨型数据集的时候有用，因为我们可能没有足够的内存来读取巨量的数据集。它在处理线上分类任务的时候同样很实用。

注意，在使用`LineraSVC`前需要处理数据，例如将数据集中化（每个特征都除以同类特征的平均值），当我们调用`StandardScaler`的时候会自动完成这个步骤。其次注意要把`loss`参数设置为`hinge`。最后，最好也把参数`dual`设置为`False`以获取最佳表现。

### 非线性SVM分类器

​	有时候数据根本就不符合线性的分割方法，自然我们也不可能使用线性分类器对齐进行分类。之前提到过，可以通过把线性模型中的权重参数也添加为特征值来进行拟合以处理非线性问题。想要用`sklearn`来执行这个操作，我们可以使用一个`Pipeline`，其包含一个`PolynomialFeatures`转换器用于进行多项式特征变换、一个`StandardScaler`用于缩放数据，以及一个`LinearSVC`：

```python
from sklearn.datasets import make_moons 
from sklearn.pipeline import Pipeline 
from sklearn.preprocessing import PolynomialFeatures
polynomial_svm_clf = Pipeline(( 
    ("poly_features", PolynomialFeatures(degree=3)), 
    ("scaler", StandardScaler()), 
    ("svm_clf", LinearSVC(C=10, loss="hinge"))
)) 
polynomial_svm_clf.fit(X, y)
```

#### 多项式核函数

​	不单单是SVM算法，所有的机器学习算法都可以添加高维多项式特征，并且这个方法是操作便捷、效果拔群的改良之一。但是如果特征的维度不够高，模型就不能处理那些本来就非常复杂的数据集。但是维度比较高时，又会因为要处理的特征实在是太多而使得模型变得非常缓慢。

​	不过幸运的是，当使用SVM时我们可以使用名为核技巧（*kernel trick*）的数学方法来获得与非常高维度时的模型相同的结果，但是并不用真的添加这么高的维度以及使用这些维度计算：

```python
from sklearn.svm import SVC 
poly_kernel_svm_clf = Pipeline(( 
    ("scaler", StandardScaler()), 
    ("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=5))
)) 
poly_kernel_svm_clf.fit(X, y)
```

​	上述代码用3阶的多项式核训练了一个SVM分类器，对应下方左图。下方右图则是使用了10阶多项式核的SVM分类器。如果你的模型过拟合，你可以减小多项式核的阶数。相反的，如果是欠拟合，你可以尝试增大它。超参数`coef0`控制了高阶多项式与低阶多项式对模型的影响。

![image-20200502202651636](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200502202651636.png)

> 我们可以使用`grid_search`来寻找更好的参数组合，先做一些粗糙的搜索，找到一个较优的参数组合之后，再在这个参数组合附近寻找最优的组合。

#### 添加相似度特征

​	另一个解决非线性问题的方法是使用名为相似度的特征。我们取定某个点为地标，然后使用一个相似度函数计算数据集中的各个点到这个点的**相似度**，以此相似度作为分类标准。

​	例如，我们在一个数据集只有一个特征$x_1$的数据集中选取两个地标$x_1=-2,x_1=1$，并选取高斯径向核函数（*Gaussian Radial Basis Function*）作为相似函数，取$\gamma=0.3$：
$$
\phi _\gamma(\bold x,\iota) = \exp(-\gamma ||\, \bold x - \iota\, ||^2)
$$
这个函数是一个钟形函数，取值范围为$(0,1)$，在离地标$\iota$非常远的地方取值为$0$，在地标处取值为$1$。

​	首先让我们来关注实例$x_1=-1$，它离第一个地标的距离是1，离第二个地标的距离是2，于是它对应的新特征是$\phi (x_2) =\exp(-0.3 \times 1^2)=0.74$，而在以另一个地标为参照的核函数中的距离对应为$\phi (x_3)=\exp(-0.3 \times 2^2)=0.3$。于是在我们的新坐标图$(x_2,x_3)$中，$x_1=-1$这个点对应的点是$(0.74,0.3)$。对$x_1$上的点进行上述操作，我们发现新的数据变得线性可分了。

![image-20200502202703902](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200502202703902.png)

​	那么我们如何选取地标呢？最简单的方法就是在每一个数据集中的地点都创建一个地标，这样就创建了许多新的维度，增大了数据被转化为线性可分数据的可能性。缺点在于一个有m个实例以及每个实例n个特征的数据集将被转换为一个有m个实例和m个特征的数据（假设我们丢弃原来的特征）。如果我们的训练集非常大，最后我们可能会有好几万个特征。

#### 高斯RBF核

​	就像之前提到的多项式特征方法一样，相似度特征方法也在机器学习算法中非常实用。然而这种方法同样非常消耗计算资源，因为我们要计算所有的额外特征。然而，之前提到的核技巧在这里仍然适用：高斯核让你可以获得同样好的结果成为可能，就像你在相似特征法添加了许多相似特征一样，但事实上，你并不需要在RBF添加它们。我们使用 SVC 类的高斯 RBF 核来检验一下

```python
rbf_kernel_svm_clf = Pipeline(( 
	("scaler", StandardScaler()), 
    ("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001))
)) 
rbf_kernel_svm_clf.fit(X, y)
```

​	这个模型有两个参数，`gamma`和`c`。其中`gamma`控制钟形曲线的宽度，增大`gamma`将会减小钟形曲线的宽度，这将导致每个实例对结果的影响范围变小，决策边界将变得更加不规则，沿着实例的分布而蜿蜒分布。减小`gamma`则会增大钟形曲线的宽度，实例对结果的影响范围变大，决策边界变得规则而连续。也就是说，*`gamma`就像我们之前提到的规范化参数，如果我们的模型出现过拟合现象我们就应该减小它，而如果我们的模型尚不足以拟合它，则应该增大`gamma`。*参数C扮演的角色其实和`gamma`差不多。

![image-20200502202729063](C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20200502202729063.png)

​	还有其他的核函数，但很少使用。例如，一些核函数是专门用于特定的数据结构。在对文本文档或者 DNA 序列进行分类时，有时会使用字符串核（String kernels）（例如，使用 SSK 核（string subsequence kernel）或者基于编辑距离（Levenshtein distance）的核函数）

>   这么多可供选择的核函数，你如何决定使用哪一个？一般来说，你应该**先尝试线性核函数**（记住`LinearSVC`比`SVC(kernel="linear")`要快得多），尤其是当训练集很大或者有大量的特征的情况下。如果训练集不太大，你也可以尝试高斯径向基核（Gaussian RBF Kernel），它在大多数情况下都很有效。如果你有空闲的时间和计算能力，你还可以使用交叉验证和网格搜索来试验其他的核函数，特别是有专门用于你的训练集数据结构的核函数。

#### 计算复杂度

​	`LinearSVC`类基于`liblinear`创建，它有许多针对线性SVM的优化算法。它不支持核技巧，不过它的计算量级几乎是随着训练集的大小以及数据的特征数量线性增加的：$O(m\times n)$。

​	如果我们要求更高的精度的话，算法需要的时间也会更长。这个精度是由容忍度参数`tol`来决定的。不过在大多数分类任务中，默认的容忍度已经足够使用了。

​	`SVC`类基于`libsvm`创建，它支持核技巧。其训练模型的时间复杂度通常介于$O(m^2\times n)$与$O(m^3\times n)$之间。这意味着当数据集变得比较大时算法变得令人窒息地慢，但是它对数据的特征值的增加并不敏感。这就是为什么我们之前说SVM方法比较适用于数据量不大，但是特征值非常多的情况。下图解释了一些`sklearn`中算法类的复杂度：

![image-20200502202803256](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200502202803256.png)

### SVM回归

​	SVM还支持线性或者非线性的回归，只要我们把原来的分类任务的目的倒置一下，把“寻找一条街，在限制边界冲突的前提下尽可能地扩大街的宽度”转换为“寻找一条街，在限制边界冲突的前提下，尽可能地让更多的实例分布在这条街上”。此时，对边界冲突的定义也发生了变化：离开大街的样本被视为引发了冲突。

​	大街的宽度由参数`epslion`控制，其值越大街道越宽，反之则越小，如下图所示。

![image-20200502202819719](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200502202819719.png)

​	向训练集中添加位于大街内的数据**不会**影响模型及其预测。，因此这个模型被认为是不敏感的。

​	我们可以使用`sklearn`中的`LinearSVR`类来完成线性SVM回归，下面的代码产生的就是上图的结果：

```python
from sklearn.svm import LinearSVR
svm_reg = LinearSVR(epsilon=1.5) 
svm_reg.fit(X, y)
```

​	要解决非线性回归的问题，我们可以使用核心化的SVM模型，注意下面的代码中`SVR`表示的其实就是`SVC`类的回归器版本。类似的还有`LinearSVR`与`LinearSVC`对应。

```python
from sklearn.svm import SVR
svm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1) 
svm_poly_reg.fit(X, y)
```

​	如果我们设置一个很大的C，规范化的程度就会很小。

![image-20200502202830224](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200502202830224.png)

### 机理

​	注意，这里我们把原来偏移项$\theta_0$称为$b$，其他的权重组成的向量称为$\bold w$。

#### 决策以及预测函数

​	线性SVM分类器模型仍然是通过计算某个实例$\bold x$的各个特征值的加权和及其偏移量的和，既决策函数$ F=\bold w^T \cdot \bold x +b$来完成分类的，如果其结果$\hat y$为正数就表明它属于类别1，否则属于类别0。
$$
\hat y = 
\begin{cases}
0 & \textrm{if } \bold w^T \cdot \bold x + b <0 \\
1 & \textrm{else}
\end{cases}
$$
​	如下图所示，在三维状态空间中实例的xy坐标分别为其特征的值（假设它只有两个特征），其z轴坐标是其决策函数$F$的值。令$F=0$得到一个平面，该平面和xy坐标平面的交线就是对实例的决策边界。当然，我们也可以通过$F=1,-1$调整对应的决策边界（如图中虚线所示）。![image-20200502202842272](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200502202842272.png)

#### 训练目标

​	我们的决策函数为$F=\bold w^T \cdot \bold x +b$，考虑其斜率，如果我们把斜率除以2，那么新的$\bold x'$要比$\bold x$大2倍（假设b为0）才能达到相同的取值。于是我们的边界就扩大了两倍，或者说，那条街道各向两侧扩展一倍，宽度变为原宽度的四倍。

​	根据我们的原则，我们这里就要尽可能的减小$||\bold w||$以获得一个较大的边界。然而如果我们想要限制边界冲突的数量，我们就需要保证决策函数能够满足对于大部分分类为0的实例输出$<-1$，而对所有分类为1的实例输出$>1$。

​	如果我们定义$t^i=-1, \textrm{where } y^i=0$，且$t^i=1, \textrm{where } y^i=1$，于是我们就把上述限制（对边界冲突的限制）转换为了$t^i\times(\bold w ^T \cdot \bold x^i + b)\geq 1$对于大部分实例成立。

​	如果我们考虑严格情况，也就是说上述的两个条件必须被严格满足，这就是硬间隔SVM分类器的目标，于是我们的任务转化为：

- 调整$\bold w$，使得$\frac{1}{2}\bold w^T \bold w$最小化
- 满足$t^i\times(\bold w ^T \cdot \bold x^i + b)\geq 1$

> 这里我们求$\frac{1}{2}\bold w^T \bold w$的最小值，而不是$||\bold x||$的最小值。首先求这两者的最小值的是等效的，其次前者的偏导数就是$\bold w$，便于计算，而后者除了计算麻烦之外，还在$\bold w=0$处不可微，不便于运算。

​	而如果我们考虑软间隔SVM分类器的目标，我们需要为每一个实例引入一个松弛变量$\zeta^i \geq 0$。$\zeta^i$衡量第i个实例究竟能在多大的程度上被我们的模型容忍引发边界冲突。于是此时我们有了两个互相冲突的任务：让松弛变量越小越好，限制边界冲突的个数；同时让$\frac12\bold w^T \cdot \bold w$越小越好，拓宽道路的边界。我们可以使用之前提到的参数`C`来调整模型的倾向性。问题的数学模式表示为：

- 调整$\bold x,b,\zeta$，使得$ \text{  } \frac{1}{2}\bold w^T \cdot \bold w + C\sum_{i=1}^{m}\zeta^i\\$最小化
- 满足 $t^i\times(\bold w ^T \cdot \bold x^i + b)\geq 1 -\zeta^i, \zeta^i\geq0$

#### 二次规划

​	至此，*hard margin*以及*Soft margin*问题都已经转换为了在线性条件限制下的二阶可导函数的最优解问题（*Quadratic Programming Problems*）。一个QP问题的通用描述形式为：

- 调整$p$，使得$\frac12 \bold p^T \cdot \bold H\cdot \bold p+ \bold f^T\cdot \bold p \\$最小化
- 满足$\bold A\cdot \bold p \leq \bold b$

$$
\textrm{where}
\begin{cases}
\bold p & \textrm{is an }n_p \textrm{-dimensional vector}(n_p = \textrm{number of parameters})\\
\bold H & \textrm{is an } n_p \times n_p \textrm{ matrix}\\
\bold f & \textrm{is an }n_p \textrm{-dimensional vector}\\
\bold A & \textrm{is an } n_c \times n_p \textrm{ matrix}(n_c = \textrm{number of constraints})\\
\bold b & \textrm{is an }n_c \textrm{-dimensional vector}\\
\end{cases}
$$

注意表达式$\bold A\cdot \bold p \leq \bold b$事实上定义了$n_c$个限制：$\bold p^T\cdot \bold a^i \leq b^i \textrm{ for } i=1,2,\ldots,n_c$，其中$\bold a^i$是一个由$\bold A$的第i列组成的向量，$b^i$是$\bold b$的第i个元素。

​	将我们的硬间隔SVM分类问题套入上式，有：

- $n_p=n+1$，其中$n$是特征的个数，$+1$是因为还有一个常数偏移项
- $n_c=m$，其中$m$是训练集中实例的个数
- $\bold H$是一个$n_p \times n_p$的单位矩阵，不过它的第一个元素为0，以忽略常数偏移量
- $\bold f= \bold 0$
- $\bold b = \bold 1$
- $a^i = -t^i \cdot \dot {\bold x}$，其中$\dot{\bold x}$等于$\bold x$，除了其常数偏移项对应的位置$\hat{\bold x_0=1}$

所以，另一种训练一个硬间隔SVM分类器方法就是：直接使用一个QP Solver（有很多现成的QP问题解决器），传递入上述的这些参数，结果向量$\bold p=(b,p_1,p_2,\ldots,p_n)$。

​	与上述情况类似的，将软间隔SVM分类的问题带入QP Solver，也可以当作训练出了一个软间隔SVM分类器。

​	然而，上面提到的这种方法是不能使用核技巧的。想要使用核技巧我们需要对问题进行另一种优化。

#### 对偶问题

​	给定一个限制优化问题，我们将之称为第一问题（*Primal Problem*）。它**有可能**可以被转化为另一个与之关联但是又有所不同的问题，我们将这个问题称呼为第二问题（*Dual Problem*）。这两个问题不一定完全等价，也不一定同解，但是只要第二问题给第一问题的解设定了一个下限，并且某些情况下第二问题和第一问题同解。

​	幸运的是，我们的SVM问题存在满足上述定义的第二问题。针对一个线性SVM问题我们有如下的第二问题：

-   调整$\alpha$，使得 $\frac{1}{2}\sum^{m}_{i=1}\sum_{j=1}^m \alpha^i \alpha^j t^it^j  {{\bold x}^i}^T \cdot \bold x^j -\sum_{i=1}^m \alpha^i$最小化
-   满足 $\alpha^i \geq 0$

​	一旦我们找到了符合条件的$\hat \alpha$，我们就可以通过下式计算第一问题的解：
$$
\label{solve}
\hat{\bold w} = \sum _{i=1}^{m} \hat{\alpha}^i t^i \bold x^i\\
\hat b = \frac{1}{n_s} \sum _{i=1}^{m} \left(1-t^i\left(\hat{\bold w}^T \cdot \bold x^i \right) \right)
$$
​		解决第二问题所需的计算资源小于第一问题，并且解决第二问题可以使用核技巧

#### 核技巧

​	假设我们现在有一个数据集，每个数据有两个特征。我们想要使用该训练集以及多项式核函数的方法来训练一个分类器或者回归器。那么，如前所述我们首先需要调整数据集，添加一些维度特征以用于训练。例如我们给出如下的函数$\phi(\bold x)$来完成对数据集的调整，将原来的一次多项式$x_1,x_2$转化为二次多项式$x_1^2,x_1x_2, x_2^2$：
$$
\phi(\bold x) = \phi\left(
\begin{pmatrix}
x_1\\
x_2
\end{pmatrix}
\right)=
\begin{pmatrix}
x_1^2\\
\sqrt{2}x_1x_2\\
x_2^2
\end{pmatrix}
$$
​	那么如果我们对$\bold a, \bold b$分别使用这个函数并且将结果做一个点积：
$$
\begin{aligned}
\phi(\bold a)^T \cdot \phi(\bold b) &=
\begin{pmatrix}
a_1^2\\
\sqrt{2} a_1a_2\\
a_2^2
\end{pmatrix}^T \cdot
\begin{pmatrix}
b_1^2\\
\sqrt{2} b_1b_2\\
b_2^2
\end{pmatrix}=a_1^2b_1^2 + 2a_1b_1a_2b_2+a_2^2b_2^2\\
&=(a_1b_1+a_2b_2)^2 = \left(
\begin{pmatrix}
a_1\\
a_2
\end{pmatrix}^T
\begin{pmatrix}
b_1\\
b_2
\end{pmatrix} \right)^2= \left( \bold a^T \cdot \bold b\right)^2
\end{aligned}
$$
我们发现，$\phi(\bold a)^T \cdot \phi(\bold b) =\left( \bold a^T \cdot \bold b\right)^2$，如果我们对整个训练集应用这个函数，我们事实上不需要真的计算每个实例$\bold x $的函数值并让他们两两做内积，而是直接使用$\left( \bold a^T \cdot \bold b\right)^2$。该方法极大的简化了数学运算，这就是核技巧的核心。

​	函数$K(\bold a, \bold b)=\left( \bold a^T \cdot \bold b\right)^2$被称为二维多项式核心。在机器学习领域中，如果函数$K(\bold a, \bold b)$满足：计算$\phi(\bold a)^T \cdot \phi(\bold b)$事实上只需要$\bold a, \bold b$参与运算，而不需要计算甚至知道$\phi$本身时，我们将这个函数称为**核函数**。以下是常见的核函数：

-   线性：$K(\bold a, \bold b)=\bold a^T \cdot \bold b$
-   多项式：$K(\bold a, \bold b)=\left( \gamma\bold a^T \cdot \bold b + r\right)^d$
-   高斯径向核函数：$K(\bold a, \bold b)=\exp \left( -\gamma||\bold a - \bold b||^2\right)$
-   Sigmoid：$K(\bold a, \bold b)=\tanh\left( \gamma\bold a^T \cdot \bold b + r\right)$ 

​	式$\ref{solve}$给出了线性SVM分类问题下如何从第二问题的解得到第一问题的解的方法。然而我们如果使用了核技巧，我们的方程最后就会含有$\phi(x)$。而观察式$\ref{solve}$我们不难发现$\hat{\bold w}$必须与$\phi(x)$具有相同的维度，而后者的维度则可能是非常大甚至无穷的，于是我们不能计算得到$\hat {\bold w}$，而没有$\hat{\mathbf w}$我们如何进行预测？我们可以把$\ref{solve}$中$\hat{\bold w}$的形式代入决策函数中，得到一个可以求解的式子：
$$
\begin{aligned}
h_{\hat{\bold x},\hat b} \left(\phi\left(\bold x^n\right)\right) &=\widehat{\bold w}^T \cdot
\phi\left(\bold x^n\right) + \hat b = \left(\sum_{i=1}^{m}\hat \alpha^it^i\phi(\bold x^i)\right)^T \cdot \phi\left(\bold x^n\right) + \hat b \\
&= \sum_{i=1}^{m}\hat \alpha^it^i \left(\phi(\bold x^i)^T \cdot \phi(\bold x^n)\right)+\hat b \\
&= \sum_{i=1}^m \hat \alpha^i t^i K\left(\bold x^i, \bold x^n\right) + \hat b\\(\hat \alpha^i >0)

\end{aligned}
$$
类似的，下面是$\hat b$的解法：
$$
\begin{aligned}
\hat{b}&=\frac{1}{n_{s}} \sum_{i=1}^{m}\left(1-t^{(i)} \widehat{\mathbf{w}}^{T} \cdot \phi\left(\mathbf{x}^{(i)}\right)\right)=\frac{1}{n_{s}} \sum_{i=1}^{m}\left(1-t^{(i)}\left(\sum_{j=1}^{m} \hat{\alpha}^{(j)} t^{(j)} \phi\left(\mathbf{x}^{(j)}\right)\right)^{T} \cdot 
\phi\left(\mathbf{x}^{(i)}\right)\right)\\
&=\frac{1}{n_{s}} \sum_{i=1}^{m}\left(1-t^{(i)} \sum_{j=1}^{m} \hat{\alpha}^{(j)} t^{(j)} K\left(\mathbf{x}^{(i)}, \mathbf{x}^{(j)}\right)\right.& \\
&\hat{\alpha}^{(i)}>0 ,  \hat{\alpha}^{(j)}>0
\end{aligned}
$$
