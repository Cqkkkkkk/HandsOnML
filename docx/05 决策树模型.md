## 决策树模型

#### 1	训练并可视化决策树

​	决策树是一颗树，使用决策树进行分类就是比对决策树节点上的条件，选择路径分支下行直到到达叶节点的过程。我们不妨先创建一颗决策树试一试，仍然使用之前的iris数据集

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
iris = load_iris()
X = iris.data[:, 2:]
y = iris.target

tree_clf = DecisionTreeClassifier(max_depth=2)
tree_clf.fit(X, y)
```

​	我们将创建的这颗决策树可视化为图片：

```python
from sklearn.tree import export_graphviz
import pydot
export_graphviz(
	tree_clf,
	out_file='iris_tree.dot',
	feature_names=iris.feature_names[2:],
	class_names=iris.target_names,
	rounded=True,
	filled=True
)

graph, = pydot.graph_from_dot_file('iris_tree.dot')
graph.write_png('iris_tree.png')
```

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200503183228996.png" alt="image-20200503183228996" style="zoom: 33%;" />

##### 1.1	预测

​	如上图所示，如果我们拿到一个新的实例，只需要对照决策树的条件逐渐下沉到叶节点，对应的分类就是该实例的分类。

​	决策树的优点之一在于，它几乎不需要数据预处理，例如标准化和中心化等等曾经对结果至关重要的操作在这里都不需要。

​	在上面的决策树中，每一个节点都有一个`sample`属性，表示在训练过程中这个节点出现了多少次。例如，根节点的`sample=150`表示训练集中有150个实例的下沉路径进过该节点。一个节点的`value`表示训练集中的不同的类别的实例经过该节点的次数。例如紫色叶节点的`value=[0,1,45]`表示经过这个节点的各个分类的实例分别为0,1,45个。在这里，表明有一个错判的例子。最后，节点的`gini`属性表示节点的`impurity`。一个节点是纯洁的（`gini=0`），如果经过它的所有实例都属于同一个类别。`gini`的计算公式为：
$$
G_i = 1 -\sum _{k=1}^n p_{i,k}^2
$$
​	`sklearn`使用CART算法，产生的决策树一定是二叉树。

![image-20200503191615519](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200503191615519.png)

​	决策树的决策边界们如上图所示。在进行第一次分隔（实线）之后，左边已经变得纯净了，于是左边不用再分隔。对右侧区间进行再分割，将其分割为两侧（虚线）。由于我们设定了`max_depth=2`，在这之后就不会进行分隔了。如果我们把`max_depth`调整为3，那么第三次分隔则如点状线段所示。

>   **白盒算法与黑盒算法**
>
>   ​	有些机器学习算法是非常具有指导意义并且容易理解其中的缘由的，例如决策树算法。这种算法我们称呼其为白盒算法，我们能很容易的直到算法为什么作出这些决策，为什么作出这种分类，究竟是哪些特征促使我们的算法作出对应的决策。
>
>   ​	然而，也有一些黑盒算法，例如神经网络和随机森林算法，尽管他们也具有非常高的性能和准确率，但是我们并不能直观的知道并解释为什么算法作出这样的决策，也不能知道究竟是那些特征在起到重要作用。例如，在使用神经网络进行人脸识别的过程中，我们很难知道究竟是人脸的什么特征在帮助我们识别这张人脸。

##### 1.2	预测概率

​	使用`predict_proba`可以获取某个实例属于对应类别的概率

#### 2	CART算法

​	`sklearn`使用分类与回归树（*Classification and Regression Tree， CART*）算法来训练决策树。其原理为，算法根据某一个特征$k$以及一个阈值$t_k$将训练集分为两部分，然后对这两个子集重复上述操作，直到达到给定深度。

​	那么特征$k$和阈值$t_k$的是如何选择的呢？算法搜索$(k,t_k)$使得产生的两个子集都更为纯净，或者说它们的`gini`最小。使满足下式的$J(k,t_k)$最小的$(k,t_k)$就是分类依据：
$$
J(k,t_k)=  \frac{m_{\textrm{left}}}{m}G_{\textrm{left}}+\frac{m_{\textrm{right}}}{m}G_{\textrm{right}}
$$

-   $G_{\textrm{left/right}}$表示两个子集的*Gini*不纯净度
-   $m_{\textrm{left/right}}$是两个子集的元素量

​	当然，我们也可以使用一些其他的条件来提供额外的终止条件，例如`min_samples_split`，`min_sam ples_leaf`，`min_weight_fraction_leaf`以及`max_leaf_nodes`。

>   如我们所见，CART算法是一个贪心算法，它并不关心长远的最小可能不纯净度，只关心目前的最小不纯净度

​	不过找到一个最优的分割树是一个NP完全问题，解决它需要$O(\exp(m))$的时间复杂度，这是我们完全不能接受的。于是我们仅仅找到一个足够好的解即可。

#### 3	计算复杂度

​	在已经训练好的决策树中对实例进行判断下沉只需要$O(\log_2(m))$的时间复杂度。

​	训练一个决策树的时间复杂度是$O(n\times m\log(m))$，其中$n$为样本数量，$m$为样本的特征数量。对于**较小**的训练集（一般指样本数在$10^3$这个量级左右，或者更少）`sklearn`可以通过提前将数据排序来加速训练，只需要设置`presort=True`即可。

#### 4	*Gini* Vs 交叉熵

​	默认情况下算法将使用*Gini*不纯洁度作为衡量方法。但是我们也可以通过设置`criterion='entropy'`来使用交叉熵不纯洁度。机器学习算法中经常使用交叉熵最为不纯净度的度量：一个集合的交叉熵是0，如果它只含有一类数据的话。其他情况下，交叉熵$H_i$符合如下公式：
$$
H_{i}=-\sum_{k=1}^{n} p_{i, k} \log \left(p_{i, k}\right) \\
p_{i, k} \neq 0
$$
​	大多数情况下，使用这两种度量标准差不了多少，并且*Gini*算起来更快，于是我们常用的方法是*Gini*。

#### 5	正则化

​	正如我们在第一章遇到的例子，如果我们不对决策树加以限制的话，它会拟合我们训练集中的全部数据，这显然是典型的过拟合现象。

​	我们将那些在训练之前参数的数量没有被确定的模型称为非参数模型（*Nonparametric model*）。注意不是因为它没有参数（它通常有很多），而是因为由于参数数量没有预先限制，模型的结构可以任意地自由拟合数据，常常造成过拟合现象。类似的，参数模型（*Parametric model*），例如线性模型，它的参数地数量是给定的，于是模型的自由度是有限的，这种模型更不容易犯过拟合的错误，不过它又可能犯不拟合的错误。

​	想要正则化决策树，除了限制决策树的深度之外，还有：

-   `min_samples_split`：在一个节点被分裂时，它至少要有多少个`sample`
-   `min_samples_leaf`：一个叶节点至少要有多少个`sample`
-   `min_weight_fraction_leaf`：一个叶节点至少要有占总体多少比例的`sample`
-   `max_leaf_nodes`：叶结点的最大数量
-   `max_features`：用于衡量如何分隔节点的特征的最大数目

增加上述`min_`的参数或者减小`max_`的参数可以正则化模型，如下图所示

![1](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200504191032369.png)

#### 6	回归

​	决策树也可以用于回归分析，只需要使用`sklearn`中的`DecisionTreeRegressor`类即可构建决策回归树。

```python
from sklearn.tree import DecisionTreeRegressor
tree_reg = DecisionTreeRegressor(max_depth=2) 
tree_reg.fit(X, y)
```

​	这颗回归树和我们之前构造的决策树之间的唯一区别在于，它预测的不是类别，而是值。而这个值就是求当前节点的所有实例的值的平均值得到的。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200504191922947.png" alt="image-20200504191922947" style="zoom: 33%;" />

​	我们可以通过增加决策树的深度来更好地拟合数据，例如：

![image-20200504192448894](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200504192448894.png)

​	CART算法也在做几乎和之前一样的工作，只不过这时它划分两个子集的凭据不再是最小化不纯净度，而是最小化$\textrm{MSE}$：
$$
J(k,t_k)=  \frac{m_{\textrm{left}}}{m}\textrm{MSE}_{\textrm{left}}+\frac{m_{\textrm{right}}}{m}\textrm{MSE}_{\textrm{right}}\\
\textrm{MSE}_{\textrm{node}}=\sum_{i \in \textrm{node}}\left(\hat y_{\textrm{node}}-y^i \right)^2 \\
\hat y_{\textrm{node}} = \frac{1}{m_{\textrm{node}}}\sum_{i \in \textrm{node}} y^i
$$
​	类似之前提到的分类算法，如果不对模型进行正则化，那么过拟合将几乎是一定的：

![image-20200504232024126](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200504232024126.png)

#### 7	不稳定性

​	决策树也具有很多缺点。首先决策树更偏好水平或者竖直的决策边界，这样使得模型对数据位置的旋转非常敏感。例如，下图中的左侧数据集中，由于数据分界是垂直的，决策树能非常准确的找出这个数据分界。但是当我们将数据旋转一定的角度变成右侧图形时，决策边界变得断断续续，出现了一些不必要的转折。尽管这两个决策树都非常好的拟合了训练集，但是右侧的的模型在实践中的表现将远低于左侧的模型。解决或者部分解决这个问题的方法是使用PCA方法（以后会提到）。

![image-20200504232736430](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200504232736430.png)

​	其次，决策树算法对数据的微小变动非常敏感，改变很少的数据就有可能得到完全不同的决策树。例如我们把*Iris*数据集中的具有最长叶宽的*Versicolor*数据移除，接着进行训练，模型将形如![image-20200504233012683](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200504233012683.png)

，对比之前的模型图我们不难发现变了不止一点点。