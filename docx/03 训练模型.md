## 训练模型

在这里之前，我们都是将机器学习算法当作黑箱来使用和处理。现在，让我们来仔细研究这些机器学习算法的内核，这能更好地帮助我们理解和使用机器学习的算法、决定使用什么样的模型，并且更好的训练模型。

### 线性回归模型

#### 模型介绍

一个线性回归模型通过计算输入的权值的和，以及一个偏差项的和来作出预测：
$$
\hat y=\theta_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n
$$
其中，$\hat y$是预测的值，$\{x_1, x_2, \ldots,x_n\}$是输入的特征值，$\{\theta_0,\theta_1,\ldots,\theta_n \}$表示模型参数。将上式写为矩阵向量的积的形式：
$$
\hat y = h_{\theta}(\bold x) = \theta^T \cdot \bold x
$$


其中：

-   $\theta$是模型的参数向量，$\theta=(\theta_0,\theta_1,\ldots,\theta_n )$。$\theta^T$是它的转置
-   $\bold x$是对应实例的特征值向量，$\bold x=(x_0, x_1,\ldots, x_n)$，其中$x_0=1$恒成立
-   $h_\theta$是预测函数

#### 训练模型

在这里，训练一个线性回归模型意味着调整模型的参数，使得其最好地匹配训练集。故而我们首先需要一个评估函数来评估我们的模型究竟有多（或者有多不）匹配训练集，常用的评估函数如RMSE函数。也就是说，现在的问题就是找到一个向量$\theta$，使得根据这个参数组得到的预测结果与真实值相差最小，或者说RMSE值最小。不过从实际操作上来说，计算MSE的最小值比计算RMSE的最小值方便，所以有时候我们也计算MSE的值并使其最小：
$$
\textrm{MSE}(X, h_\theta)=\frac{1}{m}\sum_{i=1}^{m}\left (\theta^T \cdot \bold x^i -y^i\right)
$$

##### Normal Equation

​	想要找到一个最合适的$\theta$，我们已经有了一个数学上的解析解：
$$
\hat \theta = (\bold X^T \cdot \bold X)^{-1}\cdot \bold X^T \cdot \bold  y
$$

值得注意的是，该公式的计算复杂度大约为$\textrm{O}(n^{2.4})\to\textrm{O}(n^{3})$，故而当数据量比较大时，该方法会异常的慢。在数据量达到$100000$级别时，计算量就已经增长到了令人难以接受的地步。

当然，这里的好处就是该方法比较节省内存空间。并且一旦我们使用数据完成了训练并得到了一个$\theta $，接下来的预测环节就异常迅速了，只需要使用常数级别的时间就可以完成预测操作。

##### 梯度下降

梯度下降方法是寻找最优解的一种极为优秀的方法。梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。其基本原理在于，我们可以通过逐渐的调整参数来在一个状态空间曲线中逐渐下降，最后达到损失函数的最低点。方法上来说，其计算某一点的损失函数关于参数向量$\theta$的梯度，然后其向着梯度下降最多的方向调整参数。一旦该点的梯度变为0，就表示它达到了最低点。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200425113132061.png" alt="image-20200425113132061" style="zoom:33%;" />

下降的**步长**是梯度下降方法的重要参数，我们也将其称呼为**学习率**。学习率太低时，算法需要很多很多轮梯度下降才能达到最低点，这样做将会非常消耗时间。然而如果学习率太高，它可能在曲线的两端左右跳动，可能达不到一个较为理想的最低点，甚至永远达不到叫地点。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200425113545650.png" alt="image-20200425113545650" style="zoom:33%;" />

但是，并不是所有的损失函数都像上面的函数一样**标准规范**，它可能是一个奇绝诡怪的函数形式。局部最低点的，以及平原地区的存在将可能极大的影响我们的梯度下降函数的性能。例如，在下图的左侧，算法可能会认为它已经达到了最低点，于是停止不动。在右侧，由于状态处于平原态，算法同样可能因为梯度几乎为0而认为已经达到最低点。然而，这二者都不是最优解。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200425113743646.png" alt="image-20200425113743646" style="zoom: 33%;" />

不过幸运的是，我们的MSE损失函数对于一个线性模型来说，是一个比较规范的函数。作为一个凸函数它没有局部最低点，同时它也是一个连续函数，其梯度不会忽然改变。有了这两个特性，我们使用梯度下降方法就一定能达到全局最低点，只要我们愿意等待足够多的时间。

最后，使用梯度下降算法时我们必须确定每个参数的量级处于平衡的状态。否则该算法将会变得异常缓慢。

###### 批量梯度下降方法

要使用梯度下降方法，我们要计算损失函数关于每一个模型参数$\theta_j$的偏导数：
$$
\frac{\partial}{\partial \theta_j}\textrm{MSE}(\theta) = \frac{2}{m}\sum_{i=1}^{m}(\theta^T\cdot \bold x^i - y^i)x_j^i
$$
我们可以使用如下的方程来批量计算每一个偏导数：
$$
\nabla_\theta \textrm{MSE}(\theta) = 
\begin{pmatrix}
\frac{\partial}{\partial \theta_0}\textrm{MSE}(\theta)\\
\frac{\partial}{\partial \theta_1}\textrm{MSE}(\theta)\\
\vdots \\
\frac{\partial}{\partial \theta_n}\textrm{MSE}(\theta)

\end{pmatrix}
=\frac{2}{m}\bold X^T\cdot (\bold X \cdot \theta - \bold y)
$$
注意到这个公式使用了整个训练集$\bold X$作为每一次梯度下降运算的对象，结果就是在大数据压力下该方法将会非常缓慢。不过这个还是比Normal Equation的那种情况快的多。

当我们有了指向上坡方向最大值的偏导数向量之后，我们向着相反的方向下坡即可。这意味着$\theta$要减去$\nabla_\theta \textrm{MSE}(\theta)$，减去的倍数就是我们的学习率$\eta$：
$$
\theta^{\textrm{(next step)}} = \theta - \eta\nabla_\theta \textrm{MSE}(\theta)
$$
这时，学习率$\eta$的设置至关重要。当学习率过小时，算法最终会到达解的位置，但是这可能很慢很慢；而当学习率过大时，算法将大幅度的震荡，以至于每一步都在离和理解越来越远。

我们可以使用`grid_search`来寻找一个比较合适的学习率。但是另一方面，万一算法在实验一个学习率非常低的模型，我们希望限制迭代的次数，以免我们的`grid_search`陷入低学习率的泥潭。不过如何设置这个迭代次数的上限又是一个值得研究的问题了。如果上限设置得比较小，那么最后得到的将都是一些不太拟合模型的参数。设置得太大又会很浪费时间。一个合适的解决方法是：设置一个非常大的迭代上限，但是在$\nabla_\theta \textrm{MSE}(\theta)$小于某个很小的阈值$\varepsilon$时停止迭代。

当损失函数是凸函数且其斜率不会突变时，在固定学习率的情况下其批量梯度下降有一个为$\rm O\left(\frac{1}{iterations}\right)$的收敛速度。也就是说，如果我们把$\varepsilon$除以10，那么算法就会迭代约10被的次数。

```python
eta = 0.1 # learning rate
n_iterations = 1000
m = 100
theta = np.random.randn(2,1) # random initialization
for iteration in range(n_iterations): 
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) 
    theta = theta - eta * gradients
```

###### 随机梯度下降SGD

批量梯度下降的主要问题在于它每次都是用整个训练集来计算梯度，这样就导致耗时非常严重。与之对立的方法则是随机梯度下降，它在每一步随机地选取一个训练集的实例，并且就用这一个实例来计算梯度。显然这样算法将会非常快，但是这个算法将使得我们的损失函数在状态空间中游走，有时上升有时下降， 总体上呈下降的趋势，而不是逐渐的向下达到最低值。随着时间推移它将会趋近于最低点，但是哪怕它已经达到了最低点它也不会停下来，而是会继续在状态空间中随机游走。

当损失函数是一个非常不规整的函数时，这个随机游走的性质可以帮助算法走出那些局部最低点以及平原。故而在寻找全局最低点上，随机梯度下降方法比批量梯度下降方法有效。

但是由于其随机性，它不可能最终达到某个最小值然后停下来，而是最多在某个最小值附近游走。我们可以适用一个逐渐减小的学习率来解决何时结束下降这个问题（这事实上是*模拟退火算法*）。我们把学习率随迭代次数的函数称为学习方案。如果学习率下降的太快了，我们就有可能陷在局部最低点中无法挣脱（低学习率对应着低步长），或者停在半山腰就无法继续下去了。如果学习率衰减太慢，我们有可能在“随机游走”的过程中越过了最低点，到达了另一侧的某个较高点。

下面的代码使用一个简单的`learning schedule`来实现随机梯度下降：

```python
n_epochs = 50 
t0, t1 = 5, 50 # learning schedule hyperparameters
def learning_schedule(t): 
    return t0 / (t + t1)
theta = np.random.randn(2,1) # random initialization
for epoch in range(n_epochs): 
    for i in range(m): 
        random_index = np.random.randint(m) 
        xi = X_b[random_index:random_index+1] 
        yi = y[random_index:random_index+1] 
        gradients = 2 * xi.T.dot(xi.dot(theta) - yi) 
        eta = learning_schedule(epoch * m + i) 
        theta = theta - eta * gradients
```

我们可以使用`sklearn`中的`SGDRegresser`来完成这个操作：

```python
sgd_reg = SGDRegresser(n_iter = 50, penalty=None, eta0=0.1)
sgd_reg.fit(X, y.ravel())
```

###### 小批量梯度下降

​	该方法在每次要计算梯度时，随机选取训练集中的一小组数据作为计算梯度的凭借。它相对于随机梯度下降方法的优越性除了它更为稳定的效果之外， 还有我们可以使用GPU来进行硬件计算加速。不过，它走出局部最低点和平原的能力相比SGD又稍微弱一些。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200430154300974.png" alt="image-20200430154300974" style="zoom: 50%;" />

​	不过，这些方法对模型最后的影响不是很大。所有这些算法得到的模型都差不多。

### 多元回归

如果你的数据实际上比简单的直线更复杂呢？ 令人惊讶的是，你依然可以使用线性模型来拟合非线性数据。 一个简单的方法是对每个特征进行加权后作为新的特征，然后训练一个线性模型在这个扩展的特征集。 这种方法称为多项式回归。

​	例如，我们先生成一些非线性的数据：

```python
m = 100 
X = 6 * np.random.rand(m, 1) - 3 
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)
```

显然，一条直线将永远不可能拟合这些数据。我们可以使用`sklearn`的`PolynomialFeatures`类来转化我们的数据，为每一个特征增加第二个维度作为新的特征，再用线性回归来拟合这些特征：

```python
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)
lin_reg.intercept_, lin_reg.coef_	# intercept_指示常数项，coef_指示从一次项开始的X
```

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200430161705832.png" alt="image-20200430161705832" style="zoom: 33%;" />

得到的结果还不错。

​	同时，多态回归能找出数据中特征的关联性。这是因为`PolynomialFeatures`也会考虑到所有特征的系数幂的组合。例如，存在特征$\{a,b\},\textrm{degree=3}$对应的是$a,a^2,a^3,b,b^2,b^3,ab,a^2b,ab^2$的组合。注意到，层数越高，样本的特征越高，这里要考虑的组合数就会几何式地增加。

### 学习曲线

​	高维多元回归，将会更好的拟合训练数据。但是注意到如果层数过高则会存在过拟合现象。从这里来说，当然是使用二维回归最好，这是因为我们的数据本来就是用一个二次函数叠加高斯噪声产生的。但是如果我们不知道数据的原始模型，我们如何确定最优的维度呢？

​	在第二章，你可以使用交叉验证来估计一个模型的泛化能力。如果一个模型在训练集上表现良好，通过交叉验证指标却得出其泛化能力很差，那么你的模型就是过拟合了。如果在这两方面都表现不好，那么它就是欠拟合了。这种方法可以告诉我们，你的模型是太复杂还是太简单了。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200430162619105.png" alt="image-20200430162619105" style="zoom: 43%;" />

​	另一种方法是观察学习曲线：画出模型在训练集上的表现，同时画出以训练集规模为自变量的训练集函数。为了得到图像，需要在训练集的不同规模子集上进行多次训练。下面的代码定义了一个函数，用来画出给定训练集后的模型学习曲线

```python
def plot_learning_curves(model, X, y):
	X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)
	train_errors, val_errors = [], []
	for m in range(1, len(X_train)):
		model.fit(X_train[:m], y_train[:m])
		y_train_predict = model.predict(X_train[:m])
		y_val_predict = model.predict(X_val)
		train_errors.append(mean_squared_error(y_train_predict, y_train[:m]))
		val_errors.append(mean_squared_error(y_val_predict, y_val))
	plt.plot(np.sqrt(train_errors), 'r-+', linewidth=2, label='train')
	plt.plot(np.sqrt(val_errors), 'b-',linewidth=3, label='val')
	plt.show()
```

<img src="C:\Users\1\AppData\Roaming\Typora\typora-user-images\image-20200430231050540.png" alt="image-20200430231050540" style="zoom:33%;" />

​	这幅图值得我们深究。首先，我们观察训练集的表现：当训练集只有一两个样本的时候，模型能够非常好的拟合它们，这也是为什么曲线是从零开始的原因。但是当加入了一些新的样本的时候，训练集上的拟合程度变得难以接受，出现这种情况有两个原因，一是因为数据中含有噪声，另一个是数据根本不是线性的。因此随着数据规模的增大，误差也会一直增大，直到达到高原地带并趋于稳定，在之后，继续加入新的样本，模型的平均误差不会变得更好或者更差。我们继续来看模型在验证集上的表现，当以非常少的样本去训练时，模型不能恰当的泛化，也就是为什么验证误差一开始是非常大的。当训练样本变多的到时候，模型学习的东西变多，验证误差开始缓慢的下降。但是一条直线不可能很好的拟合这些数据，因此最后误差会到达在一个高原地带并趋于稳定，最后和训练集的曲线非常接近。这两条曲线告诉我们，这是一个欠拟合的模型，在大数据集的情况下两条曲线都到达高原地区并且趋于稳定，且两曲线非常接近，误差很大。

>   如果模型在训练集上是欠拟合的，那么添加更多的样本是没有任何作用的。我们需要使用更加复杂的模型或者找到更好的特征。

​	我们再观测10维的多元回归模型：

```python
polynomial_regression = Pipeline([
  ('poly_features', PolynomialFeatures(degree=10, include_bias=False)),
  ('sgd_reg', LinearRegression())
])

plot_learning_curves(polynomial_regression, X, y)
```

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200430232509745.png" alt="image-20200430232509745" style="zoom:33%;" />

我们发现，训练集的RMSE大大减小了，初始阶段甚至没有误差，这意味着模型在训练集上拟合得非常好，也是过拟合的标志之一。不过随着训练集规模增大，两曲线最终还是会相会。但是两条曲线在稳定后又存在较大的间隔，说明模型存在过拟合现象。

>   改善模型过拟合的一种方法是提供更多的训练数据，直到训练误差和验证误差相等

>   **偏差/方差抉择**
>
>   我们的模型的主要错误可以分为三类：
>
>   -   偏差：泛化误差的这部分误差是由于错误的假设决定的。例如实际是一个二次模型，你却假设了一个线性模型。一个高偏差的模型最容易出现欠拟合。
>   -   方差：这部分误差是由于模型对训练数据的微小变化较为敏感，一个多自由度的模型更容易有高的方差（例如一个高阶多项式模型），因此会导致模型过拟合。
>   -   不可约误差：这部分误差是由于数据本身的噪声决定的。降低这部分误差的唯一方法就是进行数据清洗（例如：修复数据源，修复坏的传感器，识别和剔除异常值）。
>
>   增加模型的复杂度会显著地增加它的方差，但是减少它的偏差。反之亦然

### 线性模型的正则化

​	规范化模型有利于减少过拟合现象。

#### 岭回归

​	岭回归是正则线性回归的一种形式。它向损失函数增加了一个正则项(*Regularization term*)，其值为$\alpha \sum_{i=1}^{n}\theta_i^2$。这就会迫使学习算法不仅要使得模型拟合数据，更会使得模型的参数权重越小越好。注意这个正则项只需要在训练模型的时候添加。训练完模型之后对模型的评估阶段不需要添加这个正则项。

>   一般情况下，训练过程使用的损失函数和测试过程使用的评价函数是不一样的。除了正则化，还有一个不同：训练时的损失函数应该在优化过程中易于求导，而在测试过程中，评价函数更应该接近最后的客观表现。一个好的例子：在分类训练中我们使用对数损失（马上我们会讨论它）作为损失函数，但是我们却使用精确率/召回率来作为它的评价函数。

​	参数$\alpha$控制我们想要将模型规范化到何种程度。当$\alpha=0$是岭回归就是普通的线性回归。而如果$\alpha$很大，则所有的参数都控制在一个接近于0的范围内，以至于模型最后几乎是一条接近于0的直线，将数据均分为两半。我们常用$J$来表示岭回归的训练用损失函数：
$$
J(\theta) = \textrm{MSE}(\theta) +\frac12\alpha \sum_{i=1}^{n}\theta_i^2
$$
注意偏移参数$\theta_0$没有被规范化。

​	使用岭回归前一定要**规范化数据**（`StandardScaler`），因为该方法对数据的大小非常敏感。这个要求几乎对所有的正则线性回归都成立。我们通常称呼回归方法为岭回归，如果其在使用上述偏移量的同时还对数据进行了规范化。而如果仅仅是使用对应的偏移量，而不对数据进行规范化，我们称呼它为使用了$l_2$penalty的线性回归方法。

​	对于线性回归问题，如果我们不使用梯度下降方法而是使用Normal Equation来计算，则可以使用如下的近似形式来计算对应的回归参数：
$$
\hat \theta = \left (\bold X^T \cdot \bold X + \alpha \bold A\right )^{-1}\cdot \bold X^T \bold \cdot y
$$
​	以下是在`sklearn`中是使用岭回归的近似形式的方法：

```python
from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha=1, solver='cholesky')
ridge_reg.fit(X, y)
ridge_reg.predict([[1.5]])
```

​	以及使用随机梯度下降方法的代码。这里设置`penalty`参数就是在选择使用何种方式规范模型，设置为`l2`表示我们希望回归器为损失函数增加一个规范项，其值等于权重向量的各项的平方和的一半：

```python
sgd_reg = SGDRegressor(penalty='l2') # l2?
sgd_reg.fit(X, y.ravel())
# predict
```

#### Lasso 回归

​	Lasso回归的全称是*Least Absolute Shrinkage and Selection Operator Regression*，类似岭回归，它也向损失函数添加一个规范项，不过它使用的是一次形式的权重矩阵：
$$
J(\theta)=\textrm{MSE}(\theta) + \alpha\sum_{i=1}^{n}\mid\theta_i\mid
$$
​	Lasso回归的特点在于，**它倾向去于直接消灭那些不太重要的特征值的影响**，或者说，让它们的权重为0。该方法得到的权重向量将是一个稀疏矩阵，大多数的权重都被设置为0。

>   在 Lasso 损失函数中，批量梯度下降的路径趋向与在低谷有一个反弹。这是因为在$\theta_2=0$时斜率会有一个突变。为了最后真正收敛到全局最小值，你需要逐渐的降低学习率。

​	Lasso回归的损失函数在任何$\theta_i=0$处都是不连续的，当出现损失函数不连续的情况时我们就需要使用一个*Sub-gradient vector*来代替损失函数：
$$
g(\theta, J)=\nabla_{\theta}\textrm{MSE}(\theta)+\alpha
\begin{pmatrix}
\textrm{sign}(\theta_1)\\
\textrm{sign}(\theta_2)\\
\vdots \\
\textrm{sign}(\theta_n)
\end{pmatrix}
\textrm{where sign}(\theta_i)=
\begin{cases}
-1 & \textrm{if }\theta_i <0 \\
0 & \textrm{if }\theta_i =0 \\
+1 & \textrm{if }\theta_i >0 \\
\end{cases}
$$
​		以下是在`sklearn`中是使用Lasso回归的方法，只需要把针对岭回归的方法中的`penalty=l2`改成`penalty=l1`即可。或者，可以直接使用`sklearn`中的`Lasso`类：

```python
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X, y)
lasso_reg.predict([[1.5]])
```

#### 弹性网络

​	弹性网络是介于岭回归和Lasso回归之间的一种回归方法，它的偏移项就是上述两种方法的简单叠加：
$$
J(\theta) = \textrm{MSE}(\theta) +\frac{1-r}{2}\alpha  \sum_{i=1}^{n}\theta_i^2 + r\alpha\sum_{i=1}^{n}\mid\theta_i\mid
$$
我们可以自己设置$r$来调整模型的偏向。

​	以下是在`sklearn`中使用弹性网络的代码：

```python
from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_net.fit(X, y)
elastic_net.predict([[1.5]])
```

​	我们在使用线性回归的时候，基本上不需要考虑单纯的线性回归，而是至少要使用一点点的规范化。使用岭回归是一种不错的默认方式，不过如果我们预测只有一小部分特征是真正有用的（会对回归结果产生较大的影响的），我们可以考虑Lasso回归或者弹性网络模型。总体上来说，更推荐使用弹性网络而不是纯粹的Lasso回归，因为Lasso回归会在特征值的种类数比训练用的样本数少时发生致命的错误，或者是在有几个特征值之间是强相互关联的时候。

#### 提前终止训练

​	对于迭代学习算法，有一种非常特殊的正则化方法，就像梯度下降在验证错误达到最小值时立即停止训练那样。我们称为早期停止法。下图表示使用批量梯度下降来训练一个非常复杂的模型（一个高阶多项式回归模型）。随着训练的进行，算法一直学习，它在训练集上的预测误差（RMSE）自然而然的下降。然而一段时间后，验证误差停止下降，并开始上升。这意味着模型在训练集上开始出现过拟合。一旦验证错误达到最小值，便提早停止训练。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200909173124441.png" alt="image-20200909173124441" style="zoom: 33%;" />

​	随着迭代次数的增加，学习算法调整参数并且它在训练集上的RMSE逐渐下降。但是它在测试集上的RMSE却事实上在先下降后上升，这意味着随着训练的不断深入，模型开始出现了过拟合现象。故而在针对测试集的RMSE最低点处终止循环并以此时的模型作为最终模型是一个不错的主意。

​	不过在使用随机梯度下降和小批量下降方法时，曲线并不会是一条光滑的曲线，我们很难知道什么时候才是真正的最小值。这时候我们可以设置在迭代了一定层数（如100层）之后再在某一个最小值处终止训练。

>   随机梯度和小批量梯度下降不是平滑曲线，你可能很难知道它是否达到最小值。 一种解决方案是，只有在验证误差高于最小值一段时间后（你确信该模型不会变得更好了），才停止，之后将模型参数回滚到验证误差最小值。

​	以下是该方法的实现：

```python
from sklearn.base import clone
sgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None, learning_rate="constant", eta0=0.0005)
minimum_val_error = float("inf") 
best_epoch = None 
best_model = None 
for epoch in range(1000): 
    sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off 
    y_val_predict = sgd_reg.predict(X_val_poly_scaled) 
    val_error = mean_squared_error(y_val_predict, y_val) 
    if val_error < minimum_val_error: 
        minimum_val_error = val_error 
        best_epoch = epoch 
        best_model = clone(sgd_reg)
```

​	值得注意的是，我们初始化回归器时设置了`warm_start=True`时，当调用回归器的`fit()`函数时模型不会从头开始训练，而是从开始或者上一次结束的位置开始训练。 

#### 逻辑回归

​	如前所述，一些回归算法也可以用于分类。Logistic回归通常用于估计一个实例述语某个特定类型的概率。如果估计的概率大于50%，那么模型预测这个实例属于当前类（称为正类，标记为“1”），反之预测它不属于当前类（即它属于负类 ，标记为“0”）。 这样便成为了一个二元分类器。

##### 预测概率

​	类似线性回归模型，逻辑回归模型计算输入特征值的加权和（还要加上偏差项）。不过它不是直接输出结果，而是把结果输入`logistic()`函数进行二次加工之后进行输出：
$$
\label{13}
\hat p = h_\theta(\bold x) = \sigma(\theta^T\cdot\bold x)
$$
这个Logistic通常被记为$\sigma()$，是一个Sigmoid函数，返回一个$(0,1)$之间的值：
$$
\sigma(t) = \frac{1}{1+\textrm{exp}(-t)}
$$
，并且对于某个实例$\bold x$，在通过式$(13)$得到其属于其的分类概率之后，如果概率大于0.5就表明其属于这一类，否则就不属于这一类：
$$
\hat y = 
\begin{cases}
0 &\textrm{ if } \hat p < 0.5 \\
1 &\textrm{ if } \hat p \geq 0.5
\end{cases}
$$

##### 训练以及损失函数

​	训练一个逻辑回归模型的目标在于调整其权重参数向量$\theta$，使得这个模型能够更精确的判断概率分类。或者说，当一个实例$y=1$时我们的模型要输出尽可能高的$\hat p$，反之则是尽可能低的$\hat p$。

​	上面的这个思路由一个针对某个实例$\bold x$的损失函数来实现：
$$
c(\theta) =
\begin{cases}
-\log(\hat p)  &\textrm{ if  } y=1 \\
-log(1-\hat p) &\textrm{ if  } y=0 
\end{cases}
$$
这个损失函数的特点在于，$-\log(t)$在$t \to +0$时变得非常大，于是损失函数在模型判断错误时将会非常大。反之亦然。

​	上面的损失函数$c(\theta)$时针对训练集中的某一个实例$\bold x$的。如果我们使用批量梯度下降方法，那么全局的损失函数为：
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}\left( y^i\log(\hat p^i)+(1-y^i)log(1-\hat p^i) \right)
$$
如果我们使用随机梯度下降方法则只需随机选取一个实例完成上述操作。使用批量梯度下降则对全体训练集中的实例完成上述操作并取平均值。使用小批量下降则对所取的小批量完成上述操作并求平均值。

​	该方法没有针对Normal Equation的近似解。

​	该损失函数是连续的，其偏导数为：
$$
\frac{\partial}{\partial \theta_j}J(\theta)=\frac{1}{m}\sum_{i=1}^m
\left ( \sigma(\theta^T\cdot\bold x^i) -y^i \right)x_j^i
$$

##### 决策边界

​	我们使用鸢尾花数据集来具体实现一下逻辑回归。这个数据集是三种花(*Setosa, Versicolor, and Iris-Virginica*)的集合。

​	首先我们获取数据，只需要使用`data`中的第三列数据，由于后续要求输入一个二维数组，我们将其`reshape`成每个元素对应的实例一行，这一行只有一个元素（Petal width）的二维数组。

```python
from sklearn import datasets
iris = datasets.load_iris()
X = iris["data"][:, 3].reshape(-1, 1) 			# petal width 
y = (iris["target"] == 2).astype(np.int) 		# 1 if Iris-Virginica, else 0
```

​	接着我们训练一个逻辑回归模型：

```python
from sklearn.linear_model import LogisticRegression
log_reg = LogisticRegression() 
log_reg.fit(X, y)
```

​	我们可以使用如下的代码绘制预测概率$\hat p$关于实例花瓣宽度的函数：

```python
X_new = np.linspace(0, 3, 1000).reshape(-1, 1) 
y_proba = log_reg.predict_proba(X_new) 
plt.plot(X_new, y_proba[:, 1], "g-", label="Iris-Virginica") 
plt.plot(X_new, y_proba[:, 0], "b--", label="Not Iris-Virginica") 
# + more Matplotlib code to make the image look pretty
```

注意这里我们使用了`predict_proba`，其返回一个二维数组，两列分别表示实例属于该类、不属于该类的概率。这样就会得到每个实例的预测概率$\hat p$而不是一个预测值。如果我们直接使用`predict`，则会根据预测边界$50\%$来判断分类属性。

​	注意，之前的模型都通过建立分类器时设置参数$\alpha$来设置规范化的程度。而在逻辑回归模型中则使用另一个参数：$C$。其值越高，模型被规范化的程度就越低。

#### Softmax回归

##### 原理

​	逻辑回归直接针对多个分类类别时，我们称呼其为Softmax回归模型，或者多元逻辑回归模型。Softmax回归的思路十分简单：给定一个实例$\bold x$，模型首先给每一个类别$k$计算其对应的分数$s_k(\bold x)$，接着对这些分数调用*softmax function*（也成为*noemalized exponential*）得到实例$\bold x$属于各个类别的概率。计算$s_k(\bold x)$的方程为：
$$
s_k(\bold x)=\theta_k^T \cdot \bold x
$$
​	在我们计算出了$\bold x$在所有类别上的分数之后，我们使用如式$(20)$所示的*Softmax function*计算实例$\bold x$属于某个类别$k$的概率。该函数计算类$k$的分值的指数幂占所有的类的分值的指数幂的比例作为判断概率。
$$
\hat p_k =\sigma(\bold s(\bold x))_k =\frac{\exp (s_k(\bold x))}{\sum_{j=1}^K \exp(s_j(\bold x))}
$$

- $K$是类的数量

- $\bold s(\bold x)$是一个向量，含有对实例$\bold x$的所有类上的得分，注意与$s(\bold x)$的区别

    最后，Softmax回归将取上述$k$个类别的概率中的最大值对应的类别作为对$\bold x$的类别：

$$
\hat y = \textrm{argmax}_{k}\,\,\, \hat p_k=\textrm{argmax}_{k}\,
\left(\theta_k^T\cdot \bold x\right)
$$

>   注意，Softmax回归分类器只能一次判断实例$\bold x$属于多个类别的某一个类别，不能判断其可能属于那几个类别。或者说，它是多元分类器，但是是单输出的分类器。故而该分类器不能用于识别图片中的人物。
>

##### 训练

​	训练模型的目的是调整模型的参数使之尽可能的在对那些判断正确的实例给出更高的预测概率，而对那些判断错误的实例给出尽可能低的预测概率。我们使用一个名为交叉熵的损失函数可以达到这个目的。交叉熵常常用于衡量对一系列实例的预测类究竟与真实值有多匹配，当模型对目标类得出了一个较低的概率，其会惩罚这个模型。交叉熵常用于衡量待测类别与目标类别的匹配程度。
$$
J(\Theta) = - \frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^i\log(\hat p_k^i)\\
$$
其中，如果对于第$i$ 个实例的目标类是$k$，那么$y_k^{i}=1$。反之，$y_k^{i}=0$。可以看出，当只有两个类（$K=2$）时，此损失函数就是Logistic回归函数的损失函数。

上述损失函数的梯度向量为：
$$
\nabla_{\theta_k}J(\Theta) = \frac{1}{m}\left(\hat p_k^i - y_k^i\right)\bold x^i
$$
​	我们仍然使用`sklearn`中的`LogisticRegression()`操作完成对Softmax回归器的创建，只是需要设置参数`multi_class='multonomial'`，以及`solver='lbfgs'`，以及`C=10`。

```python
X = iris["data"][:, (2, 3)] # petal length, petal width 
y = iris["target"]
softmax_reg = LogisticRegression(multi_class="multinomial",solver="lbfgs", C=10) 
softmax_reg.fit(X, y)
```

​	这之后，我们如果想要知道一个叶长和叶宽分贝为$(5,2)$的花属于什么类别的花，只需要

```python
softmax_reg.predict([[5, 2]])
softmax_reg.predict_proba([[5, 2]])
```

就能得到预测甚至是实例属于各个类别的花的概率。







