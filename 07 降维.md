## 降维

很多机器学习的问题都会涉及到有着几千甚至数百万维的特征的训练实例。这不仅让训练过程变得非常缓慢，同时还很难找到一个很好的解，我们接下来就会遇到这种情况。这种问题通常被称为维数灾难。

幸运的是，在现实生活中我们经常可以极大的降低特征维度，将一个十分棘手的问题转变成一个可以较为容易解决的问题。例如，对于MNIST图片集来说图片四周边缘部分的像素几乎总是白的，因此你完全可以将这些像素从你的训练集中扔掉而不会丢失太多信息。同时，两个相邻的像素往往是高度相关的：如果你想要将他们合并成一个像素（比如取这两个像素点的平均值），你并不会丢失很多信息。

>   警告：降维肯定会丢失一些信息，因此即使这种方法可以加快训练的速度，同时也会让你的系统表现的稍微差一点。降维会让你的工作流水线更复杂因而更难维护。所有你应该先尝试使用原始的数据来训练，如果训练速度太慢的话再考虑使用降维。在某些情况下，降低训练集数据的维度可能会筛选掉一些噪音和不必要的细节，这可能会让你的结果比降维之前更好（这种情况通常不会发生。它只会加快你训练的速度）。

降维除了可以加快训练速度外，在数据可视化方面（或者DataViz）也十分有用。降低特征维度到2（或者3）维从而可以在图中画出一个高维度的训练集，让我们可以通过视觉直观的发现一些非常重要的信息，比如聚类。

在这一章里，我们将会讨论维数灾难问题并且了解在高维空间的数据。然后，我们将会展示两种主要的降维方法：投影和流形学习，同时我们还会介绍三种流行的降维技术：主成分分析（PCA），核主成分分析（KernelPCA）和局部线性嵌入（LLE）。

### 维数灾难

我们已经习惯生活在一个三维的世界里，以至于当我们尝试想象更高维的空间时，我们的直觉不管用了。即使是一个基本的4D超正方体也很难在我们的脑中想象出来，更不用说一个200维的椭球弯曲在一个1000维的空间里了。

这表明很多物体在高维空间表现的十分不同。比如，如果你在一个正方形单元中随机取一个点（一个`1×1`的正方形），那么随机选的点离所有边界大于0.001（靠近中间位置）的概率为0.4%（`1-0.998^2`）（换句话说，一个随机产生的点不大可能严格落在某一个维度上。但是在一个1,0000维的单位超正方体（一个`1×1×...×1`的立方体，有10,000个1），这种可能性超过了99.999999%。在高维超正方体中，**大多数点都分布在边界处**。

还有一个更麻烦的区别：如果我们在一个单位正方形中随机选取两个点，那么这两个点之间的距离平均约为0.52。而若在单位立方体中选取两个随机点，平均距离将大致为0.66。但是，在一个1000000维超立方体中随机抽取两点呢？其平均距离大概为408.25（大致$\sqrt{1000000/6}$）。这一事实意味着高维数据集有很大可能分布得非常稀疏：大多数训练实例可能彼此远离。这也意味着一个新实例可能远离任何训练实例，这使得预测的可靠性远低于我们处理较低维度数据的预测，因为它们将基于更大的推测。简而言之，*训练集的维度越高，过拟合的风险就越大*。

理论上来说，维数爆炸的一个解决方案是增加训练集的大小从而达到拥有足够密度的训练集。不幸的是，在实践中，达到给定密度所需的训练实例的数量随着维度的数量呈指数增长。如果只有 100 个特征（比 MNIST 问题要少得多）并且假设它们均匀分布在所有维度上，那么如果想要各个临近的训练实例之间的距离在 0.1 以内，我们可能需要比宇宙中的原子还要多的训练实例。

### 降维的主要方法

在我们深入研究具体的降维算法之前，我们来看看降低维度的两种主要方法：投影和流形学习。

#### 投影（Projection）

在大多数现实生活的问题中，训练实例并不是在所有维度上均匀分布的。许多特征几乎是常数，而其他特征则高度异化（如前面讨论的 MNIST）。结果，所有训练实例实际上位于（或接近）高维空间的低维子空间内。这听起来有些抽象，所以我们不妨来看一个例子。在下图中，我们可以看到由圆圈表示的3D 数据集。这事实上是一个分布接近于2D子空间的3D数据集。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911174943640.png" alt="image-20200911174943640" style="zoom: 33%;"/>

这个3D空间中的实例分布都贴近阴影所示的平面，如果我们将每个训练实例垂直投影到这个子空间上，我们就可以得到下图所示的新的数据集，这时数据的维度便降低了。注意坐标的变换。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911175118609.png" alt="image-20200911175118609" style="zoom: 33%;" />

但是，投影并不总是降维的最佳方法。在很多情况下，子空间可能会扭曲和转动，比如下图所示的着名瑞士滚动玩具数据集：

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911175210896.png" alt="image-20200911175210896" style="zoom:33%;" />

简单地将数据集投射到一个平面上（例如，直接丢弃`x3`）会将瑞士卷的不同层叠在一起，如下左图。而我们真正需要的是展开瑞士卷所获取到的类似下右图的 2D 数据集：

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911175339442.png" alt="image-20200911175339442" style="zoom: 50%;" />

#### 流形学习

瑞士卷一个是二维流形的例子。简而言之，二维流形是一种二维形状，它可以在更高维空间中弯曲或扭曲。更一般地，一个`d`维流形是类似于`d`维超平面的`n`维空间（其中`d < n`）的一部分。在我们瑞士卷这个例子中，`d = 2`，`n = 3`：它有些像 2D 平面，但是它实际上是在第三维中卷曲。

许多降维算法通过对训练实例所在的流形进行建模从而达到降维目的；这叫做流形学习。它依赖于流形猜想，也被称为流形假设，它认为大多数现实世界的高维数据集大都靠近一个更低维的流形。这种假设经常在实践中被证实。

举MNIST的例子，所有手写数字图像都有一些相似之处，例如它们都有连线组成，边界都是白色，基本在图片正中间等等。如果我们随机生成图像，那么将会只有很小一部分图像满足这些要求。而如果我们尝试创建满足上述要求的数字图像，那么我们的自由度将会远低于随机生成图像时的自由度。换言之，这些约束把数据集压缩到了叫递维度的流形中。

流形假设通常包含着另一个隐含的假设：你现在的手上的工作（例如分类或回归）如果在流形的较低维空间中表示，那么它们会变得更简单。例如，瑞士卷被分为两类：在三维空间中（图左上），分类边界会相当复杂，但在二维展开的流形空间中（图右上），分类边界是一条简单的直线。

![image-20200911180006887](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911180006887.png)

但是这个假设也不是总成立的。例如在瑞士卷空间中，假设存在决策边界$x_1 = 5$，这个决策边界在原空间中是一个非常简单的垂直平面，但是在展开的二位流形中却变得复杂了：

![image-20200911175956864](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911175956864.png)

### 主成分分析（PCA）

主成分分析（Principal Component Analysis）是目前为止最流行的降维算法。首先它找到**接近数据集分布的超平面**，然后将所有的数据都投影到这个超平面上。

#### 保留（最大）方差

在将训练集投影到较低维超平面之前，我们首先需要选择正确的超平面。下左图是一个简单的二维数据集，以及三个不同的轴（对应一位超平面）。而图右则是将数据集投影到每个轴上的结果。投影到实线上的方式保留了最大方差，选择保持最大方差的轴是很合理的因为它很可能比其他投影损失更少的信息。证明这种选择的另一种方法是，选择这个轴使得将原始数据集投影到该轴上的均方距离最小。这是就 PCA 背后的思想，相当简单。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911180329711.png" alt="image-20200911180329711" style="zoom:33%;" />

#### 主成分（Principle Componets）

PCA 寻找训练集中可获得最大方差的轴。在上图中，它是一条实线。PCA还发现了一个与第一个轴正交的第二个轴，选择它可以获得最大的残差。在这个 2D 例子中，没有选择：就只有这条*点* 线。但如果在一个更高维的数据集中，PCA 也许可以找到与前两个轴正交的第三个轴，以及与数据集中维数相同的第四个轴，第五个轴等。 定义第`i`个轴的单位矢量被称为第`i`个主成分（PC）。在图 8-7 中，第一个 PC 是`c1`，第二个 PC 是`c2`。

>   主成分的方向不稳定：如果我们稍微打乱一下训练集并再次运行 PCA，则某些新 PC 可能会指向与原始 PC 方向相反。但是，它们通常仍位于同一轴线上。在某些情况下，一对 PC 甚至可能会旋转或交换，但它们定义的平面通常保持不变。

那么如何找到训练集的主成分呢？有一种称为奇异值分解（SVD）的标准矩阵分解技术，可以将训练集矩阵$X$分解为三个矩阵$U·Σ·V^T$的点积，其中$V^T$包含我们想要的所有主成分：
$$
V^T =
\begin{pmatrix}
\vdots &\vdots&  & \vdots \\
c_1 & c_2 & \cdots & c_n \\
\vdots &\vdots&  & \vdots \\
\end{pmatrix}
$$
下面的 Python 代码使用了 Numpy 提供的`svd()`函数获得训练集的所有主成分，然后提取前两个PC：

```python
X_centered = X - X.mean(axis=0)
U, s, V = np.linalg.svd(X_centered)
c1 = V.T[:, 0]
c2 = V.T[:, 1]
```

>   警告：**PCA 假定数据集以原点为中心**。Sklearn的`PCA`类负责为我们的数据集中心化处理。但是，如果自己实现 PCA，不要忘记首先要先对数据做中心化处理。

#### 投影到`d`维空间

一旦确定了所有的主成分，你就可以通过将数据集投影到由前`d`个主成分构成的超平面上，从而将数据集的维数降至`d`维。选择这个超平面可以确保投影将保留尽可能多的方差。例如，在图2中，3D数据集被投影到由前两个主成分定义的 2D 平面，保留了大部分数据集的方差。因此，2D投影看起来非常像原始3D数据集。

为了将训练集投影到超平面上，可以简单地通过计算训练集矩阵$X$和$W_d$的**点积**，$W_d$定义为包含前d个主成分的矩阵（即由$V^T$的前d列组成的矩阵）：
$$
X_{d-proj} = X \cdot W_d
$$
例如，下面的 Python 代码将训练集投影到由前两个主成分定义的超平面上：

```python
W2 = V.T[:, :2]
X2D = X_centered.dot(W2)	# X_centered为原始数据
```

#### 使用 Scikit-Learn

Scikit-Learn 的 PCA 类使用 SVD 分解来实现，就像我们之前做的那样。以下代码应用 PCA 将数据集的维度降至两维（请注意，它会自动处理数据的中心化）：

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X2D = pca.fit_transform(X)
```

将PCA转化器应用于数据集后，可以使用`components_`访问每一个主成分（注意，它返回以PC作为**水平向量**的矩阵，因此，如果我们想要获得第一个主成分则可以写成`pca.components_.T[:,0]`）。

#### 方差解释率（Explained Variance Ratio）

另一个非常有用的信息是每个主成分的方差解释率，可通过`explained_variance_ratio_`变量获得。它表示位于每个主成分轴上的数据集方差的比例。例如，让我们看一下图 8-2 中表示的三维数据集前两个分量的方差解释率：

```python
>>> print(pca.explained_variance_ratio_)
array([0.84248607, 0.14631839])
```

这表明，84.2% 的数据集方差位于第一轴，14.6% 的方差位于第二轴。第三轴的这一比例不到1.2％，因此可以认为第三轴可能没有包含什么信息。

#### 选择正确的维度

通常我们倾向于选择加起来到方差解释率能够达到足够占比（例如 95%）的维度的数量，而不是任意选择要降低到的维度数量。当然，除非您正在为数据可视化而降低维度——在这种情况下，您通常希望将维度降低到2或3。

下面的代码在**不降维**的情况下进行 PCA，然后计算出保留训练集方差 95% 所需的最小维数：

```python
pca = PCA()
pac.fit(X)
cumsum = np.cumsum(pca.explained_variance_ratio_)	#cunsum是累加和，元数据已经从高到低排列
d = np.argmax(cumsum >= 0.95) + 1
```

你可以设置`n_components = d`并再次运行 PCA。但是，有一个更好的选择：不指定你想要保留的主成分个数，而是将`n_components`设置为 0.0 到 1.0 之间的浮点数，表明您希望保留的方差比率。

>   如果`n_components`的值没有被设置，那么将保留所有的成分。如果被设置为$(0,1)$之间的浮点数$k$，将保留方差解释率最大的若干个成分，使得它们的方差解释率之和大于$k$。如果被设置为整数$d$，则会保留$d$个维度，同时使得方差解释率最大。

```python
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X)
```

另一种选择是画出方差解释率关于维数的函数（简单地绘制`cumsum`）。曲线中通常会有一个肘部，方差解释率停止快速增长。我们可以将其视为数据集的真正的维度。在这种情况下，可以看到将维度降低到大约100个维度不会失去太多的可解释方差。

![image-20200911192236036](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911192236036.png)

#### PCA 压缩

显然，在降维之后，训练集占用的空间要少得多。例如，尝试将 PCA 应用于 MNIST 数据集，同时保留 95% 的方差。你应该发现每个实例只有 150 多个特征，而不是原来的 784 个特征。因此，尽管大部分方差都保留下来，但数据集现在还不到其原始大小的 20%！这是一个合理的压缩比率，您可以看到这可以如何极大地加快分类算法（如 SVM 分类器）的速度。

通过应用PCA投影的逆变换，也可以将缩小的数据集解压缩回784维。当然这并不会返回给你最原始的数据，因为投影丢失了一些信息（在5％的方差内），但它可能非常接近原始数据。原始数据和重构数据之间的均方距离（压缩然后解压缩）被称为*重构误差*。例如，下面的代码将 MNIST 数据集压缩到154维，然后使用`inverse_transform()`方法将其解压缩回784维。下显示了原始训练集（左侧）的几位数字在压缩并解压缩后（右侧）的对应数字。可以看到有轻微的图像质量降低，但数字仍然大部分完好无损。

![image-20200911192914438](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911192914438.png)



PCA逆变换：
$$
X_{recovered} = X_{d-proj} \cdot W_d^T
$$

#### 增量 PCA（Incremental PCA）

先前 PCA 实现的一个问题是它需要在内存中处理整个训练集以便 SVD 算法运行。幸运的是，我们已经开发了增量 PCA（IPCA）算法：我们可以将训练集分批，并一次只对一个批量使用 IPCA 算法。这对大型训练集非常有用，并且可以在线应用 PCA（即在新实例到达时即时运行）。

下面的代码将 MNIST 数据集分成 100 个小批量（使用 NumPy 的`array_split()`函数），并将它们提供给 Scikit-Learn 的`IncrementalPCA`类，以将 MNIST 数据集的维度降低到 154 维（就像以前一样）。请注意，这里必须对每个最小批次调用`partial_fit()`方法，而不是对整个训练集使用`fit()`方法：

```python
n_batches = 100
inc_pca = IncrementalPCA(n_components=154)
for X_batch in np.array_spplit(X_mnist, n_batches):
  inc_pca.partial_fit(X_batch)
X_mnist_reduced = inc_pca.transform(X_mnist)
```

或者，我们也可以使用 NumPy 的`memmap`类，它允许我们操作存储在磁盘上二进制文件中的大型数组，就好像它完全在内存中；该类仅在需要时加载内存中所需的数据。由于增量 PCA 类在任何时间内仅使用数组的一小部分，因此内存使用量仍受到控制。这可以调用通常的`fit()`方法，如下面的代码所示：

```python
X_mm = np.memmap(filename, dtype='float32', mode='readonly', shape=(m, n))
batch_size = m // n_batches
inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
inc_pca.fit(X_mm)
```

#### 随机 PCA（Randomized PCA）

Scikit-Learn 提供了另一种执行 PCA 的选择，称为随机PCA。这是一种随机算法，可以快速找到前`d`个主成分的近似值。它的计算复杂度是$O(m × d^2) + O(d^3)$，而不是$O(m × n^2) + O(n^3)$，所以当`d`远小于`n`时，它比之前的算法快得多。

要使用随机PCA，只需要在创建PCA对象时设置参数`svd_solver='randomized'`即可。

```python
rnd_pca = PCA(n_components=154, svd_solver='randomized')
X_reduced = rnd_pca.fit_transform(X_mnist)
```

### 核 PCA（Kernel PCA）

在第5章中，我们讨论了核技巧，一种将实例隐式映射到非常高维空间（称为特征空间）的数学技术，让支持向量机可以应用于非线性分类和回归。回想一下，高维特征空间中的线性决策边界对应于原始空间中的复杂非线性决策边界。

事实证明，同样的技巧可以应用于 PCA，从而可以执行复杂的非线性投影来降低维度。这就是所谓的核 PCA（kPCA）。它通常能够很好地保留投影后的簇，有时甚至可以展开分布近似于扭曲流形的数据集。

下面的代码使用 Scikit-Learn 的`KernelPCA`类来执行带有 RBF 核的 kPCA：

```python
from sklearn.decomposition import KernelPCA
rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.04)
X_reduced = rbf_pca.fit_transform(X)
```

下图为对应使用线性核（对应简单地使用PCA类）、RBF核以及Sigmoid核将瑞士卷降低到2维的情况：
![image-20200911195604530](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911195604530.png)

#### 选择一种核并调整超参数

kPCA是无监督学习算法，因此没有明显的性能指标可以帮助我们选择最佳的核方法和超参数值。但是，降维通常是监督学习任务（例如分类）的准备步骤，因此可以简单地使用网格搜索来选择可以让该任务达到最佳表现的核方法和超参数。

例如，下面的代码创建了一个两步的流水线，首先使用kPCA将维度降至两维，然后应用Logistic回归进行分类。然后它使用`Grid_SearchCV`为 kPCA 找到最佳的核和`gamma`值，以便在最后获得最佳的分类准确性：

```python
from sklearn.model_selection import GridSearchCV 
from sklearn.linear_model import LogisticRegression 
from sklearn.pipeline import Pipeline

clf = Pipeline([
        ("kpca", KernelPCA(n_components=2)),
        ("log_reg", LogisticRegression())
])
param_grid = [{
        "kpca__gamma": np.linspace(0.03, 0.05, 10),
        "kpca__kernel": ["rbf", "sigmoid"]
    }]
grid_search = GridSearchCV(clf, param_grid, cv=3)
grid_search.fit(X, y)
```

可以通过调用`best_params_`变量来查看使模型效果最好的核和超参数：

```python
>>> print(grid_search.best_params_)
{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}
```

另一种完全为非监督的方法，是选择产生*最低重建误差的核和超参数*。但是，重建并不像线性 PCA 那样容易。下图显示了原始瑞士卷 3D 数据集（左上角），并且使用 RBF 核应用 kPCA 后生成的二维数据集（右上角）。由于核技巧，这在数学上等同于使用特征映射$φ$将训练集映射到无限维特征空间（右下），然后使用线性 PCA 将变换的训练集投影到 2D。请注意，如果我们可以在缩减空间中对给定实例实现反向线性 PCA 步骤，则重构点将位于特征空间中，而不是位于原始空间中（例如，如图中由`x`表示的那样）。由于特征空间是无限维的，我们不能找出重建点，因此我们无法计算真实的重建误差。

幸运的是，可以在原始空间中找到一个贴近重建点的点。这被称为重建前图像。一旦你有这个前图像，我们就可以测量其与原始实例的平方距离。然后，我们可以选择最小化重建前图像错误的核和超参数。

想要进行这种重建，一种方法是训练一个监督回归模型，将预计实例作为训练集，并将原始实例作为训练目标。如果您设置了`fit_inverse_transform = True`，Scikit-Learn 将自动执行此操作，代码如下所示：

```python  
rbf_pca = KernelPCA(n_components = 2, kernel="rbf", gamma=0.0433,fit_inverse_transform=True)
X_reduced = rbf_pca.fit_transform(X)
X_preimage = rbf_pca.inverse_transform(X_reduced)
```

>   默认条件下，`fit_inverse_transform = False`并且`KernelPCA`没有`inverse_tranfrom()`方法。这种方法仅仅当`fit_inverse_transform = True`的情况下才会创建。

我们可以计算重建前图像误差：

```
from sklearn.metrics import mean_squared_error
mean_squared_error(X, X_preimage) 32.786308795766132
```

现在我们可以使用交叉验证的方格搜索来寻找可以最小化重建前图像误差的核方法和超参数了

#### LLE

局部线性嵌入（Locally Linear Embedding）是另一种非常有效的非线性降维（NLDR）方法。这是一种流形学习技术，但不依赖投影。简而言之，*LLE 首先测量每个训练实例与其最近邻之间的线性关系，然后寻找能最好地保留这些局部关系的训练集的低维表示*（稍后会详细介绍） 。这使得它特别擅长展开扭曲的流形，尤其是在没有太多噪音的情况下。

下面的代码使用Sklearn中的`LocallyLinearEmbedding`类来展开瑞士卷。得到的二维数据集如下图所示，瑞士卷被完全展开，实例之间的距离也保存得比较完好。但是，距离在较大的范围内的保留不太好，展开的瑞士卷的左端被挤压，而右侧的部分则被拉长。尽管如此，LLE 在对流形建模方面做得非常好。

```python
from sklearn.manifold import LocallyLinearEmbedding

lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)
X_reduced = lle.fit_transform(X)
```

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911204723865.png" alt="image-20200911204723865" style="zoom:33%;" />

##### 原理

首先，对于每个训练实例$x^i$，该算法识别其最近的$k$个邻居。然后尝试将$x_i$重构为这些邻居的线性函数。更具体地，找到权重$w_{i,j}$使得$(x^i)^2 + (\sum_{j=1}^{m}w_{i,j}x^j)^2$最小，假设当$x^j$不是$x_i$的k个最邻近时，$w_{i,j}=0$。因此，LLE的第一步表现为式中的约束优化问题，$W$为对应的权重矩阵，第二个约束简单地对每个训练实例$x^i$的权重进行归一化。公式表达为：
$$
\widehat{\mathbf{W}}=\underset{\mathbf{W}}{\operatorname{argmin}} \sum_{i=1}^{m}\left\|\mathbf{x}^{(i)}-\sum_{j=1}^{m} w_{i, j} \mathbf{x}^{(j)}\right\|^2 \\
\text { subject to }\left\{\begin{array}{ll}
w_{i, j}=0 & \text { if } \mathbf{x}^{(j)} \text { is not one of the } k \text { c.n. of } \mathbf{x}^{(i)} \\
\sum_{j=1}^{m} w_{i, j}=1 &\text { for } i=1,2, \cdots, m
\end{array}\right.
$$
接着，将训练实例投影到一个d维空间$d<n$中去，同时尽可能的保留这些局部关系。如果$z^i$是$x_i$的投影，那么我们想要让$(z^i)^2 + (\sum_{j=1}^{m}w_{i,j}z^j)^2$最小。它看起来与第一步非常相似，但我们要做的不是保持实例固定并找到最佳权重，而是恰相反：保持权重不变，并在低维空间中找到实例图像的最佳位置。请注意，$Z$是包含所有$z^i$的矩阵。公式表达为：
$$
\widehat{\mathbf{Z}}=\underset{\mathbf{Z}}{\operatorname{argmin}} \sum_{i=1}^{m}\left\|\mathbf{z}^{(i)}-\sum_{j=1}^{m} \widehat{w}_{i, j} \mathbf{z}^{(j)}\right\|
$$

### 其他降维方法

还有很多其他的降维方法，Scikit-Learn 支持其中的好几种。这里是其中最流行的：

-   多维缩放（MDS）在尝试保持实例之间距离的同时降低了维度（见下图）
-   Isomap 通过将每个实例连接到最近的邻居来创建图形，然后在尝试保持实例之间的测地距离时降低维度。
-   t-分布随机邻域嵌入（t-Distributed Stochastic Neighbor Embedding，t-SNE）可以用于降低维度，同时试图保持相似的实例临近并将不相似的实例分开。它主要用于可视化，尤其是用于可视化高维空间中的实例（例如，可以将MNIST图像降维到 2D 可视化）。
-   线性判别分析（Linear Discriminant Analysis，LDA）实际上是一种分类算法，但在训练过程中，它会学习类之间最有区别的轴，然后使用这些轴来定义用于投影数据的超平面。LDA 的好处是投影会尽可能地保持各个类之间距离，所以在运行另一种分类算法（如 SVM 分类器）之前，LDA 是很好的降维技术。

![image-20200911205917005](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911205917005.png)



















