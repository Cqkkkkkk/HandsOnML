## 集成学习与随机森林

 大多数时候，集体的智慧大于个人的智慧。集成学习指的是训练一大堆各种各样的机器学习算法，综合这些算法的结果以获取集体智慧的算法。例如，我们可以将数据集随机分割为不同的子集，然后分别用这些子集训练一个决策树分类器，对于某个实例$\mathbf{x}$，使用这些决策树分别进行一次分类决策，然后其被分类的次数最多的那一个类就是最终的分类。这就是随机森林方法。

在这一章，我们将探索一些集成学习的策略（如何将不同的机器学习模型的结果综合起来），以及如何使用随机森林。

### 投票分类器

假设我们现在有很多针对同一分类问题的机器学习模型。那么显而易见的，综合这些模型的结果将比单一模型更好。最直接的方法就是根据投票的方法，如果有3个模型认为某个实例属于A类，而7个模型认为该实例属于B类，那么根据投票的结果，我们最终认定这个实例属于B类。

投票分类器常常比其内部的最好的分类器还要好。事实上，即使它内部的分类器是一个非常弱的分类器，可能每一个分类器单独预测的结果仅仅之比随即预测高一点点，最后的投票分类器也常常能取得非常高的准确性。不过上述事实成立的前提是：有很多弱分类器，且这些分类器互不相同。

假设我们有一个硬币，投掷它有$51\%$获得正面，$49\%$获得背面。那么当我们投掷该硬币1000次，最后得到的结果中正面比反面多的概率约为$75\%$。而如果我们投掷10000次，上述概率上升到$97\%$。这是由大数定律所决定的概率规律。类似的，如果我们的投票分类器含有1000个互相独立的弱分类器，每个分类器分类正确的概率为$51\%$，那么最后的预测正确的概率也将高达$75\%$。不过，这要求每个弱分类器之间为完全独立，事实上是不可能的，毕竟它们本身是由**同一训练集**训练的；甚至它们也可能都犯同样的错误，导致最后的投票结果出错。

投票分类器通常有两种类别：

-   *Hard-voting:* 根据子分类器的分类结果进行投票，选出票数最多的结果。
-   *Soft-voting:* 根据子分类器对实例属于各个分类的概率取平均数，取最终概率最高的为结果。

>   *Soft-voting*通常准确率更高，因为这时那些非常肯定的答案有更高的权重，而非同样地“一票”。

### Bagging 和 Pasting

除了使用不同种类的分类算法来“投票”，我们还可以将数据随机打包成若干个子集，针对同一个算法来使用这些子集训练分类器。这种方法有两个分类，分别是*bagging*和*pasting*：

-   装袋（*bagging*） ：允许训练子集之间有交集，并且允许同一个子分类器针对某个数据实例进行多次学习。简单来说，这是一种有放回的采样。
-   粘贴（*posting*）：允许训练子集之间有交集，不允许实例被同一个子分类器多次使用。简单来说，这是一种无放回的采样。

当所有的分类器被训练之后，可以通过对所有分类器结果的简单聚合来对新的实例进行预测。聚合函数常常是统计模式（例如硬投票分类器），或者平均模式（对应回归任务）。每一个单独的分类器在如果在原始训练集上都是高偏差，但是聚合降低了偏差和方差。通常情况下，集成的结果是有一个相似的偏差，但是对比与在原始训练集上的单一分类器来讲有更小的方差。

#### sklearn 中的 Bagging 和 Pasting

sklearn为上述两种算法也提供了包装：`BaggingClassifier`和`BaggingRegressor`。我们可以通过如下方式建立一个分类器，其中`n_estimators`指示建立多少个同样算法的子分类器，`max_samples`指示每个分类器用多少个数据训练，`bootstrap`指示使用*bagging*还是*posting*，`n_jobs`指示使用多少个核心用于运算（`-1`表示使用所有）。

另外，如果子分类器能够给出概率预测（例如它拥有`predict_proba`方法）的话，默认使用*Soft-voting*。

```python
bag_clf = BaggingClassifier(
	DecisionTreeClassifier(), n_estimators=500,
    max_samples=100, bootstrap=True, n_jobs=-1
)
```

下图展示了单一决策树的决策边界和Bagging集成500个树的决策边界，两者都在moons数据集上训练。正如我们看到的，继承的分类比起单一决策树的分类产生的情况更好：集成有一个可比较的偏差以及一个较小的方差。换言之，其在训练集上的错误数目大致相同，但是决策边界较不规则。

*bagging*允许同一数据出现多次，故而其内部的数据偏差将会大于*pasting*的，但是这样一来不同子分类器之前的关联性又小于*pasting*的。一般来说，*bagging*是更好的方法。

#### Out-of-Bag 评价

对于Bagging来说，一些实例可能被一些分类器重复采样，但其他的有可能不会被采样。`BaggingClassifier`默认使用有放回地采样`m`个实例，`m`为训练集的大小。这就意味着只有大约$63\%$[^1]的数据被用来训练了，还剩余$37\%$的数据一次也没有被使用，这种情况被称为*Out-of-bag*，并且这些没有被使用的数据针对每一个子分类器都是不同的。

这些没有被使用的数据可以用于对这个子分类器进行验证，这样就不再需要其他的交叉验证了。在sklearn中，我么可以在创建`BaggingClassifier`时通过设置参数`obb_score=True`来使用oob数据进行验证。评估的结果通过变量`oob_score_`来显示。

```python
>>> bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,bootstrap=True, n_jobs=-1, oob_score=True)
>>> bag_clf.fit(X_train, y_train) 
>>> bag_clf.oob_score_ 
0.93066666666666664 
```

根据这个oob评估，`BaggingClassifier`可以在测试集上达到93.1%的准确率。我们使用验证集来测试准确性：

```python
>>> from sklearn.metrics import accuracy_score 
>>> y_pred = bag_clf.predict(X_test) 
>>> accuracy_score(y_test, y_pred) 
0.93600000000000005 
```

在测试集上我们得到了93.6%的准确性，足够接近了。

[^1]: $1-(\frac{99}{100})^{100} = 0.63$。随着$m$的增长，这个比率接近于$1-\exp(-1) \approx 63.2\%$

### 随机贴片与速记子空间

`BaggingClassifier`也支持采样特征。它被两个超参数`max_features`和`bootstrap_features`控制。他们的工作方式和`max_samples`和`bootstrap`一样，但这是对于特征采样而不是实例采样。因此，每一个分类器都会被在随机的输入特征内进行训练。

当你在处理高维度输入下（例如图片）此方法尤其有效。对训练实例和特征的采样被叫做随机贴片。保留了所有的训练实例（例如`bootstrap=False`和`max_samples=1.0`），但是对特征采样（`bootstrap_features=True`并且/或者`max_features`小于 1.0）叫做随机子空间。

采样特征导致更多的预测多样性，用高偏差换低方差。

### 随机森林

随机森林时决策树的一种集成，通过使用bagging方法（有时是pasting方法）进行训练，通常用`max_samples`设置训练集的大小。与建立一个`BaggingClassifier`然后把它放入*DecisionTreeClassifier*相反，我们可以使用更方便的，也是更为优化的`RandomForestClassifier`（对于回归任务，则是`RandomForestRegressor`）。

下面的代码训练了带有500个树，每个树被限制为16叶子节点的决策森林，使用所有空闲的CPU核心：

```python
from sklearn.ensemble import RandomForestClassifier
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1) 
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)
```

除了一些例外，`RandomForestClassifier`使用`DecisionTreeClassifier`的所有超参数（决定数怎么生长），把`BaggingClassifier`的超参数加起来来控制集成本身。

随机森林算法在树生长时引入了额外的随机；与在节点分裂时需要找到**最好分裂特征相反**（详见第六章），它在一个**随机的特征集**中找最好的特征。它导致了树的差异性，并且再一次用高偏差换低方差，总的来说是一个更好的模型。

#### 极端随机树

当你在随机森林上生长树时，在每个结点分裂时*只考虑随机特征集上的特征*（正如之前讨论过的一样）。相比于找到更好的特征我们可以通过使用对特征使用随机阈值使树更加随机（像规则决策树一样）。

这种极端随机的树被简称为 *Extremely Randomized Trees*（极端随机树），或者更简单的称为 *Extra-Tree*。再一次用高偏差换低方差。它还使得 *Extra-Tree* 比规则的随机森林*更快地训练*，因为在每个节点上找到每个特征的最佳阈值是生长树最耗时的任务之一。

我们可以使用 sklearn 的`ExtraTreesClassifier`来创建一个 *Extra-Tree* 分类器。它的 API 跟`RandomForestClassifier`是相同的，相似的， *ExtraTreesRegressor* 跟`RandomForestRegressor`也是相同的 API。

我们很难去分辨`ExtraTreesClassifier`和`RandomForestClassifier`到底哪个更好。通常情况下是通过交叉验证来比较它们（使用网格搜索调整超参数）。

#### 特征重要度

针对随机森林中的单一决策树，我们可以发现，重要的特征通常出现在更加靠近根部的位置，而不重要的特征会经常出现在靠近叶子节点的位置。因此我们可以通过计算一个特征在森林的全部树中出现的平均深度来预测特征的重要性。sklearn 在训练后会自动计算每个特征的重要度，我们可以通过`feature_importances_`变量来查看结果。

下面的代码展示了iris数据库中各个特征的重要性

```python
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
iris = load_iris()
rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)
rnd_clf.fit(iris['data'], iris['target'])
for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):
	print(name, score)
# sepal length (cm) 0.112492250999
# sepal width (cm) 0.0231192882825 
# petal length (cm) 0.441030464364 
# petal width (cm) 0.423357996355 
```

随机森林可以非常方便快速得了解哪些特征实际上是重要的，特别是在需要进行特征选择的时候。

### 提升

提升（Boosting，最初称为*假设增强* ）指的是可以将几个弱学习者组合成强学习者的集成方法。对于大多数的提升方法的思想就是**按顺序去训练分类器，每一个都要尝试修正前面的分类**。现如今已经有很多的提升方法了，但最著名的就是 *Adaboost*（适应性提升，是 *Adaptive Boosting* 的简称） 和 *Gradient Boosting*（梯度提升）。让我们先从 *Adaboost* 说起。

#### Adaboost

使一个新的分类器去修正之前分类结果的方法就是对之前分类结果不对的训练实例多加关注。这导致新的预测因子越来越多地聚焦于这种情况。这是 *Adaboost* 使用的技术。

举个例子，去构建一个 Adaboost 分类器，第一个基分类器（例如一个决策树）被训练然后在训练集上做预测，在误分类训练实例上的权重就增加了。第二个分类机使用更新过的权重然后再一次训练，权重更新，以此类推，详见下图。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911115351235.png" alt="image-20200911115351235" style="zoom: 33%;" />

下图显示了连续五次预测的 moons 数据集的决策边界（在本例中，每一个分类器都是高度正则化带有 RBF 核的 SVM）。第一个分类器误分类了很多实例，所以它们的权重被提升了。第二个分类器因此对这些误分类的实例分类效果更好，以此类推。右边的图代表了除了学习率减半外（误分类实例权重每次迭代上升一半）相同的预测序列。你可以看出，序列学习技术与梯度下降很相似，除了调整单个预测因子的参数以最小化代价函数之外，AdaBoost 增加了集合的预测器，逐渐使其更好。

![image-20200911115615841](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911115615841.png)

一旦所有的分类器都被训练后，除了分类器根据整个训练集上的准确率被赋予的权重外，集成预测就非常像Bagging和Pasting了。

序列学习技术的一个重要的缺点就是：**它不能被并行化**（只能按步骤），因为每个分类器只能在之前的分类器已经被训练和评价后再进行训练。因此，它不像Bagging和Pasting一样。

##### Adaboost算法

每一个实例的权重$w_i$初始都被设置为$\frac1m$。第一个分类器被训练，然后它的权重误差率$r_1$在训练集上算出。第$j$个分类器的权重误差率计算公式为$\ref{a}$，其中$\hat y_j^i$是第$j$个分类器对于第$i$个实例的预测。
$$
\label{a}
r_j = \frac{\sum_{i=1}^{m} w^i,\quad  \textrm{where} \;\hat y_j^i \not= y^i}{\sum_{i=1}^{m} w^i}
$$
随后，通过式$\ref{b}$计算出分类器的权重$j$，其中，$\eta$是超参数学习率。分类器的准确率越高，它的权重就越高。而如果它只是下才，那么它的权重就会趋近于0。如果它总是猜错（比瞎猜的正确率更低），那么它的权重将会变为负数。
$$
\label{b}
\alpha_j = \eta \log \frac{1-r_j}{r_j}
$$
接下来实例的权重按照式$\ref{c}$更新，误分类的实例的权重会被提升。接着把所有实例的权重归一化。
$$
\label{c}
w^i \leftarrow 
\begin{cases}
w^i &\textrm{if } \hat y^i = y^i \\
w^i \times \exp(\alpha_j)& \textrm{if } \hat y^i \not= y^i
\end{cases}
$$
最后，一个新的分类器通过更新过的权重训练，整个过程被重复（新的分类器权重被计算，实例的权重被更新，随后另一个分类器被训练，以此类推）。当规定的分类器数量达到或者最好的分类器被找到后算法就会停止。

为了进行预测，Adaboost通过分类器权重$j$简单地计算了所有分类器的带权和，预测类别会是权重投票中的最主要的类别。

sklearn 通常使用 Adaboost 的多分类版本 *SAMME*（*分段加建模使用多类指数损失函数* ）。如果只有两类别，那么 *SAMME* 是与 Adaboost 相同的。如果分类器可以预测类别概率（例如如果它们有`predict_proba()`），那么sklearn 可以使用 *SAMME* 中`SAMME.R`的变量（R 代表“REAL”），这种依赖于类别概率的通常比依赖于分类的更好。

```python
from sklearn.ensemble import AdaBoostClassifier
ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200,algorithm="SAMME.R", learning_rate=0.5) 
ada_clf.fit(X_train, y_train)
```

上面的代码训练了使用 sklearn 的`AdaBoostClassifier`基于 200 个决策树桩 Adaboost 分类器（正如你说期待的，对于回归也有`AdaBoostRegressor`）。一个决策树桩是`max_depth=1`的决策树，换句话说，是一个单一的决策节点加上两个叶子结点。这就是`AdaBoostClassifier`的默认基分类器。

如果你的 Adaboost 集成过拟合了训练集，你可以尝试减少基分类器的数量或者对基分类器使用更强的正则化。

#### 梯度提升

另一个非常著名的提升算法是梯度提升。与 Adaboost 一样，梯度提升也是通过向集成中逐步增加分类器运行的，每一个分类器都修正之前的分类结果。然而，它并不像 Adaboost 那样每一次迭代都更改实例的权重，这个方法是去使用新的分类器去拟合前面分类器预测的*残差* 。

让我们通过一个使用决策树当做基分类器的简单的回归例子（回归当然也可以使用梯度提升）。这被叫做梯度提升回归树（GBRT，*Gradient Tree Boosting* 或者 *Gradient Boosted Regression Trees*）。首先我们用`DecisionTreeRegressor`去拟合训练集（例如一个有噪二次训练集）：

```python
from sklearn.tree import DecisionTreeRegressor 
tree_reg1 = DecisionTreeRegressor(max_depth=2) 
tree_reg1.fit(X, y) 
```

现在在第一个分类器的残差上训练第二个分类器：

```python
y2 = y - tree_reg1.predict(X) 
tree_reg2 = DecisionTreeRegressor(max_depth=2) 
tree_reg2.fit(X, y2) 
```

随后在第二个分类器的残差上训练第三个分类器：

```python
y3 = y2 - tree_reg1.predict(X) 
tree_reg3 = DecisionTreeRegressor(max_depth=2) 
tree_reg3.fit(X, y3) 
```

现在我们有了一个包含三个回归器的集成。它可以通过集成所有树的预测来在一个新的实例上进行预测：

```python
y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3)) 
```

下图展示了这三个树的预测，以及相应的集成预测。在第一行，集成只有一个树，所以它与第一个树的预测相似。在第二行，一个新的树在第一个树的残差上进行训练。在右边栏可以看出集成的预测等于前两个树预测的和。相同的，在第三行另一个树在第二个数的残差上训练。你可以看到集成的预测会变的更好。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911171011819.png" alt="image-20200911171011819" style="zoom: 40%;" />

我们可以使用 sklean 中的`GradientBoostingRegressor`来训练 GBRT 集成。与`RandomForestClassifier`相似，它也有超参数去控制决策树的生长（例如`max_depth`，`min_samples_leaf`等等），也有超参数去控制集成训练，例如基分类器的数量（`n_estimators`）。接下来的代码创建了与之前相同的集成：

```python
from sklearn.ensemble import GradientBoostingRegressor
gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0) 
gbrt.fit(X, y)
```

超参数`learning_rate` 确立了每个树的贡献。如果你把它设置为一个很小的数，例如 0.1，在集成中就需要更多的树去拟合训练集，但预测通常会更好。这个正则化技术叫做 *shrinkage*。下图展示了两个在低学习率上训练的 GBRT 集成：其中左面是一个没有足够树去拟合训练集的树，右面是有过多的树过拟合训练集的树。

![image-20200911171322158](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911171322158.png)

为了找到树的最优数量，我们可以使用提前停止技术。最简单使用这个技术的方法就是使用`staged_predict()`：它在训练的每个阶段（用一棵树，两棵树等）返回一个迭代器。接下来的代码用 120 个树训练了一个 GBRT 集成，然后在训练的每个阶段验证错误以找到树的最佳数量，最后使用 GBRT 树的最优数量训练另一个集成：

```python
import numpy as np 
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X_train, X_val, y_train, y_val = train_test_split(X, y)
gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120) 
gbrt.fit(X_train, y_train)
errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)] 
bst_n_estimators = np.argmin(errors)	# 找到最小参数的位置
gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators) 
gbrt_best.fit(X_train, y_train) 
```

对应的结果为：

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911171926546.png" alt="image-20200911171926546" style="zoom:50%;" />

我们也可以早早的停止训练来实现早停（与上述代码使用的  先在一大堆树中训练，然后再回头去找最优数目相反）。通过设置`warm_start=True` ，使得当`fit()`方法被调用时 sklearn 保留现有树，并允许增量训练。接下来的代码在当一行中的五次迭代验证错误没有改善时会停止训练：

```python
gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)
min_val_error = float("inf")	# 浮点数最小值
error_going_up = 0 
for n_estimators in range(1, 120):    
    gbrt.n_estimators = n_estimators    
    gbrt.fit(X_train, y_train)    
    y_pred = gbrt.predict(X_val)    
    val_error = mean_squared_error(y_val, y_pred)    
    if val_error < min_val_error:        
        min_val_error = val_error        
        error_going_up = 0    
    else:        
        error_going_up += 1        
        if error_going_up == 5:            
            break  # early stopping 
```

`GradientBoostingRegressor`也支持指定用于训练每棵树的训练实例比例的超参数`subsample`。例如如果`subsample=0.25`，那么每个树都会在 25% 随机选择的训练实例上训练。你现在也能猜出来，这也是个高偏差换低方差的作用。它同样也加速了训练。这个技术叫做*随机梯度提升*。

### Stacking

本章讨论的最后一个集成方法叫做 *Stacking*（*stacked generalization* 的缩写）。这个算法基于一个简单的想法：不使用琐碎的函数（如硬投票）来聚合集合中所有分类器的预测，我们为什么不训练一个模型来执行这个聚合？下图展示了这样一个在新的回归实例上预测的集成。底部三个分类器每一个都有不同的值（3.1，2.7 和 2.9），然后最后一个分类器（叫做 *blender* 或者 *meta learner* ）把这三个分类器的结果当做输入然后做出最终决策（3.0）。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911172502322.png" alt="image-20200911172502322" style="zoom:33%;" />

为了训练这个 *blender* ，一个通用的方法是采用保持集。让我们看看它怎么工作。首先，训练集被分为两个子集，第一个子集被用作训练第一层：

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911172547623.png" alt="image-20200911172547623" style="zoom: 50%;" />

接下来，第一层的分类器被用来预测第二个子集（保持集）（详见下图）。这确保了预测结果很“干净”，因为这些分类器在训练的时候没有使用过这些实例。现在对在保持集中的每一个实例都有三个预测值。我们现在可以使用这些预测结果作为输入特征来*创建一个新的训练集*（这使得这个训练集是三维的），并且保持目标数值不变。随后 *blender* 在这个新的训练集上训练，因此，它学会了预测第一层预测的目标值。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911172741471.png" alt="image-20200911172741471" style="zoom: 50%;" />

显然我们可以用这种方法训练不同的 *blender* （例如一个线性回归，另一个是随机森林等等）：我们得到了一层 *blender* 。诀窍是将训练集分成三个子集：第一个子集用来训练第一层，第二个子集用来创建训练第二层的训练集（使用第一层分类器的预测值），第三个子集被用来创建训练第三层的训练集（使用第二层分类器的预测值）。以上步骤做完了，我们可以通过逐个遍历每个层来预测一个新的实例。

<img src="https://pimags.oss-cn-beijing.aliyuncs.com/image-20200911172908710.png" alt="image-20200911172908710" style="zoom:50%;" />

然而不幸的是，sklearn 并不直接支持 stacking ，但是你自己组建是很容易的。或者你也可以使用开源的项目例如 [brew](https://github.com/viisar/brew)。





