## 集成学习与随机森林

 	大多数时候，集体的智慧大于个人的智慧。集成学习指的是训练一大堆各种各样的机器学习算法，综合这些算法的结果以获取集体智慧的算法。例如，我们可以将数据集随机分割为不同的子集，然后分别用这些子集训练一个决策树分类器，对于某个实例$\mathbf{x}$，使用这些决策树分别进行一次分类决策，然后其被分类的次数最多的那一个类就是最终的分类。这就是随机森林方法。

​	在这一章，我们将探索一些集成学习的策略（如何将不同的机器学习模型的结果综合起来），以及如何使用随机森林。

#### 1	投票分类器

​	假设我们现在有很多针对同一分类问题的机器学习模型。那么显而易见的，综合这些模型的结果将比单一模型更好。最直接的方法就是根据投票的方法，如果有3个模型认为某个实例属于A类，而7个模型认为该实例属于B类，那么根据投票的结果，我们最终认定这个实例属于B类。

​	投票分类器常常比其内部的最好的分类器还要好。事实上，即使它内部的分类器是一个非常弱的分类器，可能每一个分类器单独预测的结果仅仅之比随即预测高一点点，最后的投票分类器也常常能取得非常高的准确性。不过上述事实成立的前提是：有很多弱分类器，且这些分类器互不相同。

​	假设我们有一个硬币，投掷它有$51\%$获得正面，$49\%$获得背面。那么当我们投掷该硬币1000次，最后得到的结果中正面比反面多的概率约为$75\%$。而如果我们投掷10000次，上述概率上升到$97\%$。这是由大数定律所决定的概率规律。类似的，如果我们的投票分类器含有1000个互相独立的弱分类器，每个分类器分类正确的概率为$51\%$，那么最后的预测正确的概率也将高达$75\%$。不过，这要求每个弱分类器之间为完全独立，事实上是不可能的，毕竟它们本身是由**同一训练集**训练的；甚至它们也可能都犯同样的错误，导致最后的投票结果出错。

​	投票分类器通常有两种类别：

-   *Hard-voting:* 根据子分类器的分类结果进行投票，选出票数最多的结果。
-   *Soft-voting:* 根据子分类器对实例属于各个分类的概率取平均数，取最终概率最高的为结果。

>   *Soft-voting*通常准确率更高，因为这时那些非常肯定的答案有更高的权重，而非同样地“一票”。

#### 2	打包数据

​	除了使用不同种类的分类算法来“投票”，我们还可以将数据随机打包成若干个子集，针对同一个算法来使用这些子集训练分类器。这种方法有两个分类，分别是*bagging*和*pasting*：

-   *bagging:* *bootstrap aggregating*的简称，允许训练子集之间有交集，并且允许同一个子分类器针对某个数据实例进行多次学习。 
-   *posting:* 允许训练子集之间有交集，不允许实例被同一个子分类器多次使用。

​	`sklearn`为上述两种算法也提供了包装：`BaggingClassifier`和`BaggingRegressor`。我们可以通过如下方式建立一个分类器，其中`n_estimators`指示建立多少个同样算法的子分类器，`max_samples`指示每个分类器用多少个数据训练，`bootstrap`指示使用*bagging*还是*posting*，`n_jobs`指示使用多少个核心用于运算，`-1`表示使用所有。另外，如果子分类器能够给出概率预测的话，默认使用*Soft-voting*。

```python
bag_clf = BaggingClassifier(
	DecisionTreeClassifier(), n_estimators=500,
    max_samples=100, bootstrap=True, n_jobs=-1
)
```

​	*bagging*允许同一数据出现多次，故而其内部的数据偏差将会大于*pasting*的，但是这样一来不同子分类器之前的关联性又小于*pasting*的。一般来说，*bagging*是更好的方法。

##### 2.1	未被使用的数据

​	在上面的例子中，我们允许同一数据出现多次，并且一次选择100个数据。这就意味着只有大约$63\%$[^1]的数据被用来训练了，还剩余$37\%$的数据一次也没有被使用，这种情况被称为*Out-of-bag, oob*，并且这些没有被使用的数据针对每一个子分类器都是不同的。

​	这些没有被使用的数据可以用于对这个子分类器进行验证，这样就不再需要其他的交叉验证了。通过设置参数`obb_score`参数就可以使用这些数据进行验证。

[^1]: $1-(\frac{99}{100})^{100} = 0.63$。随着$m$的增长，这个比率接近于$1-\exp(-1) \approx 63.2\%$

