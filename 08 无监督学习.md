## 无监督学习

尽管目前机器学习的应用层面主要集中在监督学习上，但是大多数的数据仍然是无标签的，我们有原始数据集$\bold X$，但是我们没有它对应的标签$\bold y$。无监督学习有巨大的潜力值得我们挖掘。

在上一章中我们讨论了降维的方法，这就是无监督学习的一种重要应用。本章中我们还将讨论下列的无监督学习方法：

-   聚类算法：将相似的实例划分到某一个聚类里，实现对实例的分类。
-   异常检测：其目标在于学习什么是*正常* 的数据，并且判断实例是否处于正常状态
-   密度估算：用于估算某个产生数据集的随机过程的概率密度函数。这个方法也常被用于异常检测，那些处于非常小概率区间内的实例很可能是异常数据。

### 聚类算法

聚类算法通过某些特定的特征识别实例的类别，将实例划分为聚类，或者说划分为内部相似的组。与分类算法不同的是，聚类算法是一种无监督学习算法，没有数据标签的概念。

举Iris数据集的例子，假设训练集数据没有标签。如图9.1所示，肉眼可见的我们能将其分成两个聚类，而聚类算法则能根据两个特征分辨出右上角的长条状分布仍然可以被区分成两个聚类。这是因为还有两个其他的特征没有在此处列出来，而聚类算法则能很清楚的识别出这些特征的不同。换言之，聚类算法能合理利用所有的特征。

![image-20200912162058376](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200912162058376.png)

#### K-均值聚类（K-Means）

考虑形如图9-2的无标签数据集，我们可以很轻易地人为将数据划分为五组。K-均值聚类算法是一个用于分类这种数据集的简单有效的算法，只需要几次迭代就能得到不错的结果。

通过下述代码创建形如9-2的数据集：

```python
from sklearn.datasets import make_blobs
import numpy as np
blob_centers = np.array([
	[0.2, 2.3],
	[-1.5, 2.3],
	[-2.8, 1.8],
	[-2.8, 2.8],
	[-2.8, 1.3]
])
blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])

X, y = make_blobs(n_samples=2000, centers=blob_centers,
                  cluster_std=blob_std, random_state=7)
```

![image-20200912163623077](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200912163623077.png)

我们可以使用sklearn提供的接口来训练一个K-均值聚类分类器，它会试图找到每个聚类的中心，并且把每一个实例都划分到某个聚类中去：

```python
from sklearn.cluster import KMeans
k = 5
kmeans = KMeans(n_clusters=k)	# 创建5个聚类
y_pred = kmeans.predict(X)		
```

我们可以通过`y_pred`，或者`labels_`成员来获取数据具体被分到了那些类别：

```python
>>>y_pred 
array( [ 4, 0, 1, . . . , 2, 1, 0] , dtype=int32)
```

我们还可以通过`cluster_centers_`成员来获取算法找到的5个聚类中心的坐标：

```python
>>> kmeans.cluster_centers_
array([[- 2. 80389616, 1. 80117999], [0. 20876306, 2. 25551336], 
       [- 2. 79290307, 2. 79641063], [- 1. 46679593, 2. 28585348], 
       [- 2. 80037642, 1. 30082566]])
```

如果我们把这个聚类算法的决策边界画出来，我们可以得到形如9-3的图：

![image-20200912165719227](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200912165719227.png)

大部分的实例都被分到了合适的聚类中，不过靠近决策边界的那一部分数据则存在较多的误分类现象。事实上，当两个聚类较为接近时，K-均值聚类算法确实表现得不那么出色。

上述将每一个实例都划分到某个具体的聚类的方法，称为**硬聚类**（*Hard clustering*）。与之对应的，我们可以给每个实例针对每个聚类打一个分数，这个分数可以是实例到聚类中心的距离，或者是一个经相似度函数得到的值（例如高斯RBF函数），这种方法被称为**软聚类**（*Soft clustering*）。在`Kmeans`类中，`transform()`函数测量每个实例到所有聚类的中心的距离：

```python
>>> kmeans.transform(X_new)
array([[2. 81093633, 0. 32995317, 2. 9042344, 1. 49439034, 2. 88633901],
       [5. 80730058, 2. 80290755, 5. 84739223, 4. 4759332, 5. 84236351],
       [1. 21475352, 3. 29399768, 0. 29040966, 1. 69136631, 1. 71086031],
       [0. 72581411, 3. 21806371, 0. 36159148, 1. 54808703, 1. 21567622]])
```

##### 算法实现

刚开始时，随机地在数据空间中放置$k$个聚类中心。接着按照距离[^1]将所有的实例划分到这$k$个聚类中去，然后在这k个聚类中，每个聚类内部都通过某种方式计算出一个中心点，使得这个中心点到聚类内部的所有点的*距离* 最小，或者说所有点的*中心*，更新这个中心点。重复上述步骤直到中心点不再移动。具体过程见9-4

![image-20200912171920175](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200912171920175.png)

算法是一定会收敛的，但是却不一定会收敛到最佳的位置，它很有可能收敛到局部最小值。是否会出现这样的情况取决于聚类中心的起始位置，不同的起始位置其实对应了不同的收敛结果，形如9-5：

![image-20200912172310834](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200912172310834.png)

对于聚类中心的初始化，我们有一些其他策略来改善这个情况。

##### 初始化聚类中心

*直接添加中心*

如果我们恰好已经知道聚类中心的位置，那么我们可以人为将其添加到算法中：

```python
good_init = np.array([[- 3, 3], [- 3, 2], [- 3, 1], [- 1, 2], [0, 2]])
kmeans = KMeans(n_clusters=5, init=good_init, n_init=1)
```

*多次运行*

另一种解决方案就是运行多运行几次这个算法，每次都随机地初始化聚类中心，保留最优的解。随机初始化的次数由`n_init`参数决定，其默认值是10。不过对于一组无标签的数据，算法如何检测某一组解是不是*最优* 的呢？

算法使用*模型惯性*（*model’s inertia*），也就是每个实例到离它最近的聚类中心的平均平方距离来衡量解的优越性。我们可以通过`inertia_`成员变量来访问这个值。同时`score()`函数返回模型惯性的相反数。之所以取相反数，是为了维护sklearn数据评估中*大数据表示模型更好* （*greater is better*）的原则。

*K-means++*

这是一种改进过的K-平均算法，它使用一种更为智能的聚类中心初始化方法，使得每个聚类中心都尽可能和其他聚类中心相隔更远。这一改进使得K-平均算法更容易规避局部极值点，并且由此带来的计算量增加也是值得的——使用这种方法可以减少迭代轮数，最终简化计算。

其初始化算法为：

-   随机选取*一个* 位置放置聚类中心$c^1$
-   对于一个新的聚类中心$c^i$，以概率公式$D(\bold x^i)^2 / \sum_{j=1}^m D(\bold x^j)^2$选择一个实例$\bold x^i$，其中$D(\bold x^i)$是实例$\bold x^i$和已经选择好的、离$\bold x^i$最近的中心点之间的距离。将$c^i$放置在$\bold x^i$处。这个概率分布使得离聚类中心更远的点更有可能被选取。
-   重复第二步直到选择好了$k$个中心点

事实上，`Kmeans`类默认使用这种初始化方式。如果我们想要强制其使用比较原始地随机初始化方法，我们需要设置参数`init='random'`。不过通常没有人会这么干。

##### 加速K-均值聚类以及小批量K-均值聚类

有许多距离计算是完全没有必要且可避免以加速算法的。根据三角不等式以及跟踪实例和中心之间的距离的上下界，我们可以简化诸多计算。`Kmeans`类默认使用这种初始化方式。如果我们想要强制其计算所有的距离，我们需要设置超参数`algorithm=full`。不过绝对没有人会这么干。

针对大数据集，我们也可以使用小批量K-均值聚类。该方法可以使用小批量的数据在一个轮次中缓慢地移动中线点，而非一次使用全部的数据。这种情况下训练速度增加了三到四倍，同时避免了内存不够存储数据地情况。Sklearn提供了名为`MiniBatchKmeans`类，我们可以像使用普通`Kmeans`类那样使用它：

```python
from sklearn.cluster import MiniBatchKMeans
minibatch_kmeans = MipythonniBatchKMeans(n_clusters=5)
minibatch_kmeans.fit(X)
```

如果内存不够，可以参考上一章提到的方法。

小批量K-均值聚类算法虽然更快，但是它地性能有略微地降低

##### 找到合适的聚类数量$k$

设置合适的聚类数量对算法的效果有至关重要的影响，如9-7所示，错误的聚类数量将带来灾难性的分割：

![image-20200912193035238](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200912193035238.png)

这件事情还远远没有我们想象的那么简单。选择一个使得模型惯性最小的$k$并不是合适的做法，在这个例子中就能看到，我们找不到这样的一个点。这是因为模型惯性将随着$k$增大而不断减小，显而易见的，点的数量增加了，平均距离自然就减少了。

![image-20200912193700217](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200912193700217.png)

不过模型惯性的也能给我们一些参考，一开始惯性下降的很快，迎来拐点之后则变得缓慢下降。我们可以在这个拐点附近选择合适的值作为聚类数，例如$k=4$。

除此之外，还可以多花费一些计算资源，通过*轮廓分数*（*Silhouette score*）来选择合适的$k$。轮廓分数指的是*平均轮廓系数*（*Mean silhouette coefficient*），一个实例的平均轮廓系数的计算方式为$F=(b-a)/\max(a,b)$，其中$a$是该实例到本聚类中的其他实例的平均距离，$b$是离本聚类最近的其他聚类中的实例到这个实例的平均距离。该值取值范围为$(-1, 1)$，接近$+1$表示该实例确实在本聚类中呆的很好，且离其他聚类很远。接近$0$表示它离聚类的边界比较接近。而接近$-1$表示这个实例可能被分配到了错误的聚类。

想要计算轮廓分数，可以使用Sklearn的`silhouette_score()`函数：

```python
>>>from sklearn. metrics import silhouette_score
>>>silhouette_score( X, kmeans. labels_)
0. 655517642572828
```

下图展示了不同的$k$对应的轮廓分数。这次数据的说服力变强了许多。

![image-20200912195154817](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200912195154817.png)

#### K-均值聚类的局限性

除了之前提到的局限性之外，K-均值聚类算法还有一些缺陷。它在聚类有不同的大小、密度，甚至非球形的分布情况出现时表现都不尽如人意。例如，下图展示了其部分缺陷：

![image-20200912195900790](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200912195900790.png)

>   在使用K-均值聚类之前，我们需要**将数据清洗并标准化**。

#### 使用聚类进行图像分割

图像分割任务指的是将图像分为若干片段的任务。在图像语义分割任务中，所有属于同一类事物的像素点将被分到同一个片段。例如，在一个自驾车视觉系统里，一个行人的图像的所有像素都将被划分到同一个聚类，并且这个聚类将包含所有的行人。而在图像实例分割任务中，所有属于某一个事物的像素都将被分到某个类别，此时不同的行人对应的类别将不同。

在这里，我们将简单地尝试一下图像色彩分割，假设同一事物上的像素点总是有相近的颜色。

首先，我们采用`matploylib.image`中的`imread`函数来读取一张图片的数据：

```python
from matplotlib.image import imread
image = imread(os.path.join(" images", " unsupervised_learning", " ladybug. png"))
image.shape	# (533, 800, 3)
```

图像将以一个3D数组的形式来表示，第一维的大小是图像的高度，第二维的大小是图像的宽度，第三维的大小则是颜色原色的数量（例如在这里，是红黄蓝三原色）。下面的代码将数据处理成一个有3列的数组，使用K-均值聚类来进行色彩图片分割：

```python
X = image.reshape(- 1, 3)
kmeans = KMeans(n_clusters=8).fit(X)
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
segmented_img = segmented_img.reshape(image.shape)
```

输出如下图所示：

![image-20200912235223552](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200912235223552.png)

#### 使用均值聚类进行数据处理

聚类算法可以用于进行降维，其可以说是监督学习算法的数据处理部分的重要环节。我们举数字训练集的例子，这是一个更为小、简单的MNIST数据集，含有1797张8×8的灰度图像，表示数字0-9。首先，读入这个数据集，并将它划分为训练集和测试集：

```python
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

X_digits, y_digits = load_digits(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits)
```

接着采用逻辑回归模型：

```python
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
```

并在测试集上评估这个模型的准确性：

```python
>>> log_reg.score(X_test, y_test)
0.9688888888888889
```

得到的准确率约为96.9%。现在，让我们使用K-均值聚类方法预处理数据，看看能不能获得更好的效果。注意，虽然这里只有10种数字，但是每种数字对应的写法可能有多种，故而设置较多的聚类数是一个更好的选择，例如50。

```python
pipeline = Pipeline([
   (" kmeans", KMeans(n_clusters=50)), 
   (" log_reg", LogisticRegression()),
])
pipeline.fit(X_train, y_train)
pipeline.score(X_test, y_test)`	# 0. 9777777777777777
```

令人惊讶的是，我们将错误率降低了约30%。

 我们还可以使用网格搜索的方法来寻找如何设置聚类数$k$最好，并且这里不需要用到复杂的评估模型，只要最终的结果好，那么这个$k$就是最好的。

```python
from sklearn.model_selection import GridSearchCV

param_grid = dict(kmeans__n_clusters=range(2, 100))
grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)
grid_clf.fit(X_train, y_train)
>>> grid_clf.best_params_
{' kmeans__n_clusters': 99} 
>>> grid_clf.score(X_test, y_test)
0.9822222222222222
```

#### 使用聚类算法进行半监督学习

有时数据集中有大量的无标签数据，同时还存在小部分的有标签数据，这时我们可以使用半监督学习来学习数据集。我们仍然使用上面提到的数字数据集，但是只是用前50个实例进行训练（假设我们只知道前50个数据的标签，其余的数据都没有标签）：

```python
n_labeled = 50
log_reg = LogisticRegression()
log_reg.fit(X_train[: n_labeled], y_train[: n_labeled])
```

这时，模型的精准度大大降低了：

```python
>>> log_reg.score(X_test, y_test)
0.8333333333333334
```

为了提高模型的表现，我们可以使用半监督学习的方法。先用聚类算法将数据集分类为50个聚类（基于我们已经有的50个标签），接着对于每一个聚类，找到离中心点最近的图像，称这些图像为*代表图像*（*Representative images*）。

```python
k = 50
kmeans = KMeans(n_clusters=k)
X_digits_dist = kmeans.fit_transform(X_train)
representative_digit_idx = np.argmin(X_digits_dist, axis=0)
X_representative_digits = X_train[representative_digit_idx]
```

这些代表图像如下所示：

![image-20200913001055710](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200913001055710.png)

接着我们人为标注这些图像的标签：

```python
y_representative_digits = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])
```

现在我们的数据集重新有了50个实例标签，与之前的随机实例标签不同，现在每一个标签都代表一个聚类，使用这些标签进行训练得到的结果将会远比之前的好：

```python
>>> log_reg = LogisticRegression() 
>>> log_reg.fit(X_representative_digits,y_representative_digits) 
>>> log_reg.score(X_test, y_test)
0.9222222222222223
```

如果我们将聚类内部那些无标签的数据打上标签，接着用所有的数据进行训练，那么得到的结果将会更好，这种方式被称为生成标签（*Label propagation*）：

```python
y_train_propagated = np.empty(len(X_train), dtype=np.int32)
for i in range(k): 
  y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]
>>> log_reg = LogisticRegression() 
>>> log_reg.fit(X_train ,y_train_propagated) 
>>> log_reg.score(X_test, y_test)
0.9333333333333333
```

不过这样又有点过犹不及，因为那些处于聚类边缘的实例很可能被我们错误地打上了标签，以致误导了分类器。如果我们只把那些最靠近中心的20%的实例打上对应聚类的标签，那么效果将会更好：

```python
percentile_closest = 20
X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
for i in range(k): 
  	in_cluster = (kmeans.labels_ == i)
	cluster_dist = X_cluster_dist[in_cluster]
	cutoff_distance = np.percentile(cluster_dist, percentile_closest)
	above_cutoff = (X_cluster_dist > cutoff_distance)
	X_cluster_dist[in_cluster & above_cutoff] = - 1
    
partially_propagated = (X_cluster_dist ! = - 1)
X_train_partially_propagated = X_train[partially_propagated]
y_train_partially_propagated = y_train_propagated[partially_propagated]

>>> log_reg = LogisticRegression( ) 
>>> log_reg. fit( X_train_partially_propagated, y_train_partially_propagated) 
>>> log_reg. score( X_test, y_test)
0.94
```

##### 主动学习（Active Learning）

想要继续改善模型和训练集，我们可以进行几轮主动学习。主动学习指的是结合算法与人类的力量，人类人为的提供某些实例的标注来帮助改善模型的方法。最常用的主动学习是不确定性抽样（*Uncertainty sampling*）。该方法：

-   模型基于现有的标注好的数据集进行训练，接着将模型用于预测那些没有被标注的数据
-   选出模型最不确定的未标注实例（例如概率最低的哪一个实例），人工进行标注
-   重复上述步骤直到模型表现不再提升。

### DBSCAN

DBSCAN全称*Density-based spatial clustering of applications with noise*，是一种基于连续区域密度的聚类探测算法。与均值聚类几乎只针对球形聚类不同，DBSCAN可以探测到任意形状的聚类。其工作的原理为：

-   对每个实例$\bold x$，算法计算有多少个实例处于$\bold x$附近，或者说有多少个实例到$\bold x$的距离小于某个$\epsilon$。这个以$\bold x$为圆心，$\epsilon$为半径的区域称为实例$\bold x$的$\epsilon$临近。
-   如果一个实例$\bold x$在它的$\epsilon$临近区域内至少有`min_samples`个实例（包括它自身），那么$\bold x$就是一个核实例（*core instance*）。换句话说，核实例是那些分布在实例密集区域的实例。
-   所有处于某个核实例的临近区域内的实例属于同一个聚类。由于某个核实例也可能处于另外一个实例的邻近区域内，这样可以形成基于密度的聚类。
-   如果一个实例$\bold x$既不是核实例，也没有除了他自己之外的临近实例，那么这个实例就是一个异常实例。

基于上述做法，该算法在聚类内部实例高密度，且聚类之间被低密度区域分割时表现得非常出色。不过当聚类之间的实例密度差异非常大时，该算法几乎不可能划分出合适的聚类。Sklearn提供了`DBSCAN`类作为接口，我们不妨在月亮数据集上尝试一下：

```python
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

X, y = make_moons(n_samples=1000, noise=0.05)
dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(X)
```

所有实例的标签都存储在了`labels_`成员变量中：

```python
>>> dbscan.labels_
array([0, 2, - 1, - 1, 1, 0, 0, 0, . . . , 3, 2, 3, 3, 4, 2, 6, 3])
```

注意到有些实例被分类为`-1`，这种实例被认为是异常实例。

可以通过`core_sample_indices_`获取所有核实例的编号，或者通过`components_`获取所有核实例的具体位置。

```python
>>> dbscan.core_sample_indices_
array([0, 4, 5, 6, 7, 8, 10, 11, ..., 992, 993, 995, 997, 998, 999])
>>> dbscan.components_
array([[- 0.02137124, 0.40618608], 
       [- 0.84192557, 0.53058695], 
       ...
       [- 0.94355873, 0.3278936],
       [0.79419406, 0.60777171]])
```

这时的聚类如9-14左侧所示，它把很多正常的实例划分为了一茶馆，同时还划分出了7个聚类，实在是不太行。不过如果我们调整邻近区域的大小，增大$\epsilon$的值，情况会好很多，如9-14右侧所示。

![image-20200913121601778](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200913121601778.png)

不过，`DBSCAN`类没有`predict`接口，它不能在训练之后用于判断某个新实例是否属于某个聚类，这是由它的算法特性决定的。

如果我们想要在训练完成之后将其用于处理新实例，我们可以使用基于K-临近算法的`KNeighborsClassifier`类：

```python
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])	# 用核实例重新训练
```

现在，我们可以调用`KNeighborsClassifier`的`predict`或者`predict_proba`接口。

### 高斯混合模型（*Gaussian mixture*）

高斯混合模型（*Gaussian mixture model, GMM*）是一种概率模型，其假设实例的分布为某些类型未知、参数未知的高斯分布的组合。

最简单的GMM可以通过`GaussianMixture`类实现，我们必须提前知道高斯分布的数量$k$，数据集$\bold X$被认为由下述概率过程产生：

-   对于每个实例，随机在k个聚类中选择一个聚类。选择第$j$个聚类的概率由其权重$\Phi^i$决定。第$i$个实例选择的聚类的编号记作$z^i$。
-   如果$z^i=j$，意味着第i个实例被分配到了第j个聚类，并且位置$\bold x^i$是通过一个服从均值为$\bold u^j$且协方差矩阵为$\bold \Sigma^j$的高斯分布产生的。该分布记作$\bold x \sim N(\bold u^j, \bold \Sigma ^j)$

下图阐述了随机变量之间的条件独立结构：

![image-20200914121024611](https://pimags.oss-cn-beijing.aliyuncs.com/image-20200914121024611.png)

上图中：

-   圆圈表示随机变量。
-   方块表示某些常量（例如模型的参数）。
-   大正方形称为盘子（*Plate*），其表示内部的元素出现过多次，右下角的数字$m$指示重复的次数。
-   一共有$m$个随机变量$z^i$以及$m$个随机变量$\bold x^i$。也一共有$k$个平均值$\bold u^j$以及$k$个协方差矩阵$\bold \Sigma^j$。还有一个包含所有$k$个权重向量的权重矩阵$\bold \Phi$
-   每个变量$z^i$都是由$\bold \Phi$的类别分布产生的，而每个变量$\bold x^i$则是由普通分布产生的，这个分布如前所述，是一个服从均值为$\bold u^j$且协方差矩阵为$\bold \Sigma^j$的高斯分布。
-   实线表示条件独立性。例如，产生随机变量$z^i$的分布就和权重矩阵$\bold \Phi$密切相关。
-   波浪线表示一个*开关*， 后者的选取范围由前者决定。
-   虚线表示值是未知的。在本例中，只有$\bold x^i$的值是已知的。

讲了一大堆原理，不如来看看在Sklearn中高斯混合模型是如何是用的吧：

```python
# 产生一些数据
X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)
X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))
X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)
X2 = X2 + [6, -8]
X = np.r_[X1, X2]
y = np.r_[y1, y2]

from sklearn.mixture import GaussianMixture
gm = GaussianMixture(n_components=3, n_init=10)
gm.fit(X)
```

涉及太多概率论内容，暂时不想看了。



















