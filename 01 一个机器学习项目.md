## 一个机器学习项目

在这里，我们将通过一个实践项目来具体的介绍机器学习的方法及其应用。

#### 1	操作流程

-   我们最好获取一些真实的数据来完成我们的项目，而不是使用一些玩具数据和项目。

-   确定要完成的问题是什么，确定对应的学习方法。

-   选择一个效用函数。对于回归问题，我们经常使用$RMSE$方法作为效用函数：
    $$
    RMSE(\bold X,h)=\sqrt{\frac 1m \sum_{i=1}^{m}\left( h(x^i)-y^i \right)^2}
    $$
    其值表示我们预测的值与真实值之间的差的平方和的平方根。 

注意，在上面的效用函数中的一些名词解释：

-   $m$表示使用该方法的样本的样本容量
-   $x^i$表示一个实例的所有特征中的第$i$个特征（当然，范围不包含标签值）
-   $y^i$是$x^i$的标签
-   $\bold X$是一个含有所有的实例，以及这些实例的特征向量的矩阵。每一个实例及其对应的特征向量对应这个矩阵的一行，例如$\bold X^i$就是$(x^i)^T$
-   $h$就是我们的预测函数，当我们给这个函数输入一个实例的特征$x^i$，它返回一个对其标签的预测值$\hat y^i=h(x^i)$
-   $RMSE(\bold X, h)$是基于预测函数的一个损失函数

#### 2	创建一个测试集

​	我们使用如下的数据集，其获取方式为：

```python
import os
import tarfile
from urllib import request
import pandas as pd

download_root = "https://raw.githubusercontent.com/ageron/handson-ml/master/"
housing_path = "datasets/housing"
housing_url = download_root + housing_path + '/housing.tgz'

def fetch_data(housing_url, housing_path):
	if not os.path.isdir(housing_path):
		os.makedirs(housing_path)
	tgz_path = os.path.join(housing_path, 'housing.tgz')
	request.urlretrieve(housing_url, tgz_path)
	housing_tgz = tarfile.open(tgz_path)
	housing_tgz.extractall(path=housing_path)
	housing_tgz.close()

def load_data(housing_path):
	csv_path = os.path.join(housing_path, 'housing.csv')
	return pd.read_csv(csv_path)	# 返回pd.DataFrame格式的对象
```

##### 2.1	预览数据集

*Head*

​	在获取了数据之后，我们可以使用`.head()`函数获取数据的前五个实例来进行初步的观察：

```python
>>> data.head()
   longitude  latitude  ...  median_house_value  ocean_proximity
0    -122.23     37.88  ...            452600.0         NEAR BAY
1    -122.22     37.86  ...            358500.0         NEAR BAY
2    -122.24     37.85  ...            352100.0         NEAR BAY
3    -122.25     37.85  ...            341300.0         NEAR BAY
4    -122.25     37.85  ...            342200.0         NEAR BAY
```

*Info*	

​	或者，我们可以使用`.info()`函数来获取对数据集的总体描述：

```python
>>> data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 10 columns):
 #   Column              Non-Null Count  Dtype  
---  ------              --------------  -----  
 0   longitude           20640 non-null  float64
 1   latitude            20640 non-null  float64
 2   housing_median_age  20640 non-null  float64
 3   total_rooms         20640 non-null  float64
 4   total_bedrooms      20433 non-null  float64	# 这意味着有数据缺失
 5   population          20640 non-null  float64
 6   households          20640 non-null  float64
 7   median_income       20640 non-null  float64
 8   median_house_value  20640 non-null  float64
 9   ocean_proximity     20640 non-null  object 	# 非数字，从csv读取的，必定是字符串
dtypes: float64(9), object(1)
memory usage: 1.6+ MB
```

*Value_counts*	

​	观察上面的最后一个属性我们发现，它的类型是一个`object`对象，这意味着它可能是任何一种类型的Python对象。不过由于这个数据是读取自`csv`文件的，于是它必定是一个字符串。我们可以通过`[]`符号获取数据集中所有数据关于某个属性的值，并且可以通过`.value_counts()`获取这一列的值的统计数据，例如：

```python
>>> data['ocean_proximity'].value_counts()
<1H OCEAN     9136
INLAND        6551
NEAR OCEAN    2658
NEAR BAY      2290
ISLAND           5
Name: ocean_proximity, dtype: int64
```

*Describe*

​	我们也可以通过`.describe()`函数获得数据集的总体统计数据：

```python
data.describe()
          longitude      latitude  ...  median_income  median_house_value
count  20640.000000  20640.000000  ...   20640.000000        20640.000000
mean    -119.569704     35.631861  ...       3.870671       206855.816909
std        2.003532      2.135952  ...       1.899822       115395.615874
min     -124.350000     32.540000  ...       0.499900        14999.000000
25%     -121.800000     33.930000  ...       2.563400       119600.000000
50%     -118.490000     34.260000  ...       3.534800       179700.000000
75%     -118.010000     37.710000  ...       4.743250       264725.000000
max     -114.310000     41.950000  ...      15.000100       500001.000000
```

这里只显示了一部分，如果想要观看所有数据，可以设置`pd.options.display.width = 0`。举海拔为例子，其中有$20640$个数据，平均值为$-119$，最小值为$-123.35$，有$25\%$的数据值在$-121.8$之下，有$50\%$的数据值在$-118.4$之下，有$75\%$的数据值在$-118.0$以下。

*Figure*

​	另外，我们还可以绘制简易的图形来观察数据的分布情况：

```python
import matplotlib.pyplot as plt
data.hist(bins=50, figsize=(20, 15))
plt.show()
```

​	上图中的`hist`函数将对数据集中的每一类数据都调用一次`plt.hist`函数以获取数据的分布情况，最后将这些图标综合在一张大图中。

##### 2.1	伪“随机”

​	创建一个测试集在原理上很简单：随机的从数据集中选取一些数据，通常选取原数据集的$20\%$。例如：

```python
def split_train_test(data, test_raito):
	shuffled_indices = np.random.permutation(len(data))
	test_size = int(len(data) * test_raito)
	test_indices = shuffled_indices[:test_size]
	train_indices = shuffled_indices[test_size:]
	return data.iloc[train_indices], data.iloc[test_indices]	# iloc：横着来
```

​	但是这通常不是最好的办法，因为如果我们重新运行这个程序，那么它产生的训练集将会是一个**新**的训练集，而不是之前的集合。随着时间的推移，我们的机器学习算法最终会得以观察到整个数据集，这样训练集就失去了意义。

​	故而，我们可以使用一个特定的随机种子来使程序每次都随机地产生相同的数据集合作为测试集。但是它并没有解决这个问题：如果我们更新了一部分数据集呢？如果我们根据数据的存储地址编号以及随机得到的一个地址编号来访问数据，但是我们在中间插入了一个数据导致这之后的数据全都发生了移位呢？我们需要给每一个数据实例一个**独一无二的编号**，按照这个编号来随机的获得测试集。

*Reset_index*

​	我们初始的数据集合当然不会有一个独一无二的编号来完成这个操作，于是我们需要给他加上一个。使用`data_with_index = data.reset_index()`函数，我们将原数据中的表格加入一列，将原来的*index*转换为一个新的列，同时生成一个从0开始的整数序列代替原来的*index*。

```python
#	data.info
0        -122.23     37.88  ...            452600.0         NEAR BAY
1        -122.22     37.86  ...            358500.0         NEAR BAY
...          ...       ...  ...                 ...              ...
```

```python
#	data_with_index.info
0          0    -122.23  ...            452600.0         NEAR BAY
1          1    -122.22  ...            358500.0         NEAR BAY
...      ...        ...  ...                 ...              ...
```

​	`Scikit-Learn`提供了一些函数来将数据集划分为多个子集的方法。最简单的方法是`train_test_split`，它也有一个`random_state`参数来接受一个随机种子，一个`test_size`参数来接受测试集的比例，返回两个集合，第一个是训练集，第二个是测试集。

##### 2.2	统计偏差

​	上面我们提到的都是单纯的在样本中寻求完全的随机选择，这是建立在数据集非常大的前提下的。如果我们的数据集不够大，例如一共$1000$个人的数据中，有$20\%$的男性，$80\%$的女性，那么我们更应该分层地随机抽取样本加入测试集，而不是在整体中抽取样本加入测试集。我们希望我们抽出来的那部分人能够更好的代表整个人群，故而我们不能容忍如此巨大的统计偏差。

​	例如，我们如果要使用数据集中的收入中位数这个数据，首先我们需要**将连续的数据离散化**，将这些数据划分进不同的区间。在这里我们能发现，大多数的收入中位数都分布于$(2,5)$之间，但是仍然有一小部分数据超出了这个范围。我们在划分范围时必须要注意每个范围都需要有较多的数据才能尽可能地避免统计误差，所以我们不能在这里再划分一片新的范围给这些少数者了。我们将其直接全部归到数据的两端，高于6的那一小部分视为5：

```python
data['income_cat'] = np.ceil(data['median_income'] / 1.5)
data['income_cat'].where(data['income_cat'] < 5, 5.0, inplace=True)
# where(cond, other, inplace=True)中，将不满足cond的数据替换为other
# 如不设置inplace=True则需要重新赋值
```

​	接着，我们就可以使用`sklearn`中的`StratifiedShuffledSplit`类了，它可以用于将数据分割成测试集与训练集。通过设置`n_splits`可以设置分割为多少组训练/测试集，用于交叉验证。创建完对象之后，调用对象的`.split(X, Y)`函数可以获得训练集与测试集，其中X指示待分割的数据集，Y指示数据集的数据标签。

```python
from sklearn.model_selection import StratifiedShuffleSplit
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in split.split(housing, housing['income_cat']):
	strat_train_set = housing.loc[train_index]
	strat_test_set = housing.loc[test_index]
```

>   Pandas中的*loc*与*iloc*
>
>   -   `iloc[n]`用于按行选择数据，其选择的顺序基于行在数据中出现的位置。`n`必须是一个数字，`iloc[n]`表示访问从头开始的第*n+1*行。
>   -   `loc[index]`也用于按行选择数据，其选择的对象基于行的标签名字。`index`可以是任何对象，一般来说可以是数字，但是也可以在针对某一列`set_index`之后使用这一列进行索引。
>       -   `index`如果为一个列表，那么将选择多组数据
>       -   `index`之后还可以加一个列表`attrs`，表示要引用`index`中数据的`attr`中给出的属性

​	我们可以通过下面的代码检验这个分层抽样是否合理：

```python
print(data['income_cat'].value_counts() / len(data))
print(strat_train_set['income_cat'].value_counts() / len(strat_train_set))
print(strat_test_set['income_cat'].value_counts() / len(strat_test_set))	
```

#### 3	初步挖掘数据

##### 3.1	数据可视化

​	如果我们对样本中某两个特征之间的关联或者比较感兴趣，在深入探究之前我们可以做一些图形来获取直观感受。例如，在这个例子中我们的样本有经度和纬度，我们可以用这两个数据来做一个散点图看一看数据的分布：

```python
housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1)
#	alpha表示透明度，选用较低的透明度有利于观察密度分布
```

​	我们来画一个元素更多一点的图像：圆圈的半径表示该地区的人口数量（操作`s`），颜色表示房价（操作`c`）。我们使用一个已经定义好的*color map*（操作`cmap`），它的名字是`jet`：

```python
 housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.4, s=housing['population']/100, c='median_house_value', cmap=plt.get_cmap('jet'))
```

​	这张图告诉我们房价和地点非常有关，那么我们可能可以使用一些聚类算法来检测一些主要的聚类，然后使用一些新的特性来测算数据的距离。

##### 3.2	寻找关联

​	对于一个比较小的数据集合，我们可以很容易地计算出它的标准相关系数，只要在任何一对属性上使用`corr()`函数：

```python
corr_matrix = housing.corr()	# 求各个系数之间的关联度
print(corr_matrix['median_house_value'].sort_values())	
# 求median_house_value与其他系数的关联度，按照大小排序
```

​	标准相关系数的取值范围是$[-1,1]$，绝对值越大相关性越强，为正则是正相关，为负则是负相关。

#### 4	清洗数据

​	首先，我们将训练集中的标签全部去掉：

```python
housing = strat_train_set.drop("median_house_value", axis=1) # drop创造副本，不改变原值
housing_labels = strat_train_set["median_house_value"].copy()
```

##### 4.1	处理缺失值

​	首先，我们的数据通常有一些是有部分缺失特征值的。要解决个问题，我们可以：

-   不使用那些有缺失值的样本，将他们从样本空间中剔除，对应`dropna`
-   不使用那些含有缺失值的特征种类，对应`drop`。注意这里`axis=1`表示处理列，否则表示行。
-   使用某种办法补齐缺失的特征值对应`fillna`，注意要设置`inplace=True`。

```python
housing.dropna(subset=["total_bedrooms"]) 				# option 1
housing.drop("total_bedrooms", axis=1)					# option 2
median = housing["total_bedrooms"].median()				# 计算当前栏的中位数	
housing["total_bedrooms"].fillna(median, inplace=True)	# option 3，用某个数填充缺失个值
```

*处理数字：Imputer*

​	对于处理缺失值，我们可以使用`sklearn.preprocessing`中的`Imputer`类来方便的完成缺失值相关的操作:

```python
from sklearn.impute import SimpleImputer				
imputer = SimpleImputer(strategy="median")				# 创建一个对象，策略是使用中位数
housing_num = housing.drop('ocean_proximity', axis=1)	# 中位数只能对数字求，先去掉那些不是数字的特征
imputer.fit(housing_num)								# 求数据集的全部特征对应的中位数
# 特征的中位数的值储存在imputer.statictics_里
```

​	接着，我们就可以用这个`Imputer`的`transform`操作获得一个补齐了缺失值的训练集。这么做主要是因为后面可能新加入的数据有新的缺失项目，这样做就可以避免很多麻烦的操作。

```python
X = imputer.transform(housing_num)
```

​	得到的结果X是一个`Numpy`数组，储存的是已经整理好的数据。我们可以再将其转换为一个`pandas`的`DataFrame`结构：

```python
housing_tr = pd.DataFrame(X, columns=housing_num.columns)
```

*处理文字*	

​	注意，我们之前把非数字的特征值全都舍弃了，那么我们如何处理这些文字类型的特征值呢？方法是我们将这些文字特征值转换为数字特征值，使用`OrdinalEncoder`：

```python
from sklearn.preprocessing import OrdinalEncoder
encoder = OrdinalEncoder()
housing_cat = housing[["ocean_proximity"]] # 使用一个二维数组，主要是为了后面的要求
housing_cat_encoded = encoder.fit_transform(housing_cat) # 要求输入一个二维数组
#	housing_cat_encoded = array([3, 3, 3, ..., 1, 1, 1])
```

​	但是使用这个方法也有一个坏处，那就是机器学习算法会认为1和2之间的差距比1和4之间的小，但是这是与实际情况不符合的。解决问题的办法就是使用`OneHotEncoder`，给每一个种类都创造一个二进制的值，对应二进制为1表示它属于这个种类，并且一个实例只能有一个二进位为1，它不可能同时有两个标签。

```python
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
housing_cat_1hot = encoder.fit_transform(housing_cat)
```

​	这里的`housing_cat_1hot`将是一个压缩矩阵，它只会存储数据中非零元素的位置，而不会真的创建一个巨大的存储矩阵用于存储对应的数据。我们可以像使用真正的矩阵一样使用它，不过如果我们一定要将它转换为通常的那种形式，可以使用它的`toarray()`成员函数。

##### 4.2	自定义数据清理

​	我们可以通过自定义一个新类，并为它编写`fit()`、`transform()`以及`fit_transform()`函数来实现数据清理方法的自定义。后者可以通过让类继承`TransformerMixin`来直接获得。如果我们还将`BaseEstimator`作为基类的话，还可以直接获得`get_params()`以及`set_params()`这两个在超参数调参中很有用的函数。

​	例如，我们想要在数据中增添一些新的数据：

```python
rooms_ix, bedrooms_ix, popultaion_ix, household_ix = 3, 4, 5, 6

class CombinedAttributeAdder(BaseEstimator, TransformerMixin):
	def __init__(self, add_bedrooms_per_room=True):
		self.add_bedrooms_per_room = add_bedrooms_per_room

	def fid(self, X, y=None):
		return self  # Nothing to do

	def transform(self, X, y=None):
		rooms_per_household = X[:, rooms_ix] / X[:, household_ix]
		population_per_household = X[:, popultaion_ix] / X[:, household_ix]
		if self.add_bedrooms_per_room:
			bedrooms_per_room = X[:, bedrooms_ix] / X[: household_ix]
			return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]
		else:
			return np.c_[X, rooms_per_household, population_per_household]
			# 注意是np.c_[]，不是()

attr_adder = CombinedAttributeAdder(add_bedrooms_per_room=False)
housing_extra_attrs = attr_adder.transform(housing.values)

```

>   ​	有关`X[:, num]`的解释：这是`ndarray`新引入的用法，将其分为两部分来看，逗号左边是没有数据的`:`，表示任意行。逗号右边为`num`，表示从`0`开始的第`num`列。于是上述表述的含义就是数组`X`的所有数据的第`num`列组成的数据。

>   ​	`np.c_[a, b, c, ...]`将括号中的数组连接起来，返回一个`ndarray` 

​	下面是另一种使用`FunctionTransformer`的方法。定义一个函数用于增加数据，在构造`Pipeline`的时候根据这个函数构造`FunctionTransformer`对象作为参数。

```python
def add_extra_features(X, add_bedrooms_per_room=True):
	rooms_ix, bedrooms_ix, popultaion_ix, household_ix = 3, 4, 5, 6
	rooms_per_household = X[:, rooms_ix] / X[:, household_ix]
	population_per_household = X[:, popultaion_ix] / X[:, household_ix]
	if add_bedrooms_per_room:
		bedrooms_per_household = X[:, bedrooms_ix] / X[:, household_ix]
		return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_household]
	else:
		return np.c_[X, rooms_per_household, population_per_household]
    
def add_attrs(housing):
	attr_adder = FunctionTransformer(
		add_extra_features, validate=False, kw_args={'add_bedrooms_per_room': False}
	)
	# 注意我们之前一直是把data当作ndarray而非DataFrame处理的，这里要类型转换
	housing_extra_attrs = attr_adder.fit_transform(housing.values)
	housing_extra_attrs = pd.DataFrame(
		housing_extra_attrs,
		columns=list(housing.columns) + ["rooms_per_household", "population_per_household"],
		index=housing.index  # 这一句是啥意思
	)
	return housing_extra_attrs    
```



##### 4.3	数据缩放

​	我们得到的数据几乎一定是不标准的：它们各自有不同的取值范围，例如$(0,19)$或者$(2000,100020)$。而如果这些数据的取值范围之间的差别太大，那么我们的机器学习算法就不能取得很好的效果。有两种办法标准化我们的数据：*min-max scaling*, *standardization*。

*min-max scaling*

​	这种方法把所有的数据都变成取值范围在$(0,1)$之内的值。该操作通过减去数据中的最小值，再除以此时数据中的最大值来完成。`sklearn`提供了一个名为`MinMaxScaler`的函数来完成这个操作，特别的，如果我们不想要$(0,1)$而是想要点别的，还可以给这个函数提供一个`frature_range`参数来修改参数范围。

*standardization*

​	该方法将所有数据减去数据的平均值（因此标准化值的均值始终为零），然后将数据除以方差，以使得所得到的分布具有单位方差。这个缩放方法不会将值绑定到某个确定的范围，这对于某些机器学习算法可能有一些挑战（例如，神经网络通常期望输入值介于0到1之间）。但是标准化方法的优点在于它几乎不会收到那些超出寻常情况的数据的影响，而上面的最小-最大缩放则会受到很大的影响（它基于最小、最大值，这二者都是很容易受到异常数据影响的）。`sklearn`提供了一个名为`StandardScaler`的函数来完成这个操作。

```python
from sklearn.preprocessing import StandardSacler
a = [[1], [2], [3]]	# 注意应把这三个数据放到一列，而不是一行
scaler = StandardScaler()
std_a = scaler.fit_transform(a)
```

>   注意，数据缩放应只针对训练集，不是整个数据集

##### 4.4	总结

​	上面提到的所有操作都可以被整合在一个操作里，我们可以使用`sklearn`提供的`Pipeline`函数来完成这一个系列的数据处理。`Pipeline`可以在初始化时接受一系列的参数，要求除了最后一步的参数之外，其他的所有参数都必须要有`fit_transform`成员函数，最后一个参数可以只有`fit`成员函数。当我们调用`Pipeline`的`fit`操作时，它将会依次调用它给定参数的`fit_transform`函数，并将返回的结果作为下一个函数的输入。最后，它调用末参数的`fit`函数并最终返回对象。

​	名字可以随便取。

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
# 首先我们处理数据中的数字部分
num_pipeline = Pipeline([
	('imputer', SimpleImputer(strategy='median')),
	('attribs_adder', FunctionTransformer(add_extra_features, validate=False)),
	('std_scaler', StandardScaler())
])
num_attribs = list(housing_num)
cat_attribs = ['ocean_proximity']
full_pipeline = ColumnTransformer([
	('num', num_pipeline, num_attribs),
	('cat', OneHotEncoder(), cat_attribs)
])
housing_prepared = full_pipeline.fit_transform(housing)
```

​	至此，我们已经完成了数据处理工作。

#### 5	选择并训练一个模型

​	在准备好了数据之后，接下来的工作就异常简单了。假设我们先试一试`sklearn`提供的线性回归模型`LinearRegression`：

```python 
lin_reg = LinearRegression()
lin_reg.fit(train_housing_data_prepared, train_housing_labels)
```

​	我们可以使用$RMSE$来观察预测数据与真实数据之间的标准差分布：

```python
from sklearn.metrics import mean_squared_error
housing_predictions = lin_reg.predict(train_housing_data_prepared)
lin_mse = mean_squared_error(housing_predictions, train_housing_labels)
lin_rmse = np.sqrt(lin_mse)
# lin_rmse 即为标准差
```

​	观察发现，这个方法的标准差高达$68000$，显然并不是一个足够好的方法。这种情况一般被称为*UnderFitting*，表明在当前模型下这些特征不能很好的被用于预测。解决的最好办法是**换一个更为强力的模型**。

​	然后我们再试一试决策树回归器方法：

```python
tree_reg = DecisionTreeRegressor()
tree_reg.fit(train_housing_data_prepared, train_housing_labels)
```

我们惊讶的发现，这个方法得出来的模型，其标准差为0，出现了极其严重的过拟合现象。

##### 5.1	使用交叉验证更好地评估

​	要提高我们的评估函数的效用，我们可以将原来的一整个训练集划分为多个子训练集以及子验证集。用这些小的训练集训练模型，然后用这些子验证集验证他们并进行修改。我们可以使用`sklearn`中的`cross_validation`来完成这个操作。在下面给出的代码中，我们使用了一个称为*K-fold*交叉验证的方法，它随机的将训练集划分为10个子集，用这些子集训练并评估决策树10次，每次都用1个子集作为验证集，其余9个子集为训练集。函数返回的结果是一个包含这10次评估得分的数组：

```python
tree_reg = DecisionTreeRegressor()
tree_reg.fit(train_housing_data_prepared, train_housing_labels)
scores = cross_val_score(tree_reg, train_housing_data_prepared, train_housing_labels, scoring='neg_mean_squared_error', cv=10)
rmse_scores = np.sqrt(-scores)
print(rmse_scores)
print(rmse_scores.mean())
print(rmse_scores.std())
```

​	这时候，我们就可以发现决策树模型比线性回归模型更垃圾。使用交叉验证我们能更准确地知道模型地准确度。*linear Regression model: 68972.377566*  ,  *Decision Tree model:  71199.4280043* 

​	我们不妨再试一试随机森林模型：

```python
from sklearn.ensemble import RandomForestRegressor
forest_reg = RandomForestRegressor()
forest_reg.fit(housing_prepared, housing_labels)
```

​	这一回就好多了，其得分为$50000$左右，但是我们注意到他的得分和其RMSE值$18000$还有很大的差距，这说明过拟合现象是仍然存在的。

##### 5.2	进一步优化我们的模型

###### 5.2.1	Grid Search

​	给定一定的参数，函数`GridSearchCV`将自动尝试给出的一系列参数组合，并且给出所有组合的对应交叉验证的结果。

```python
from sklearn.model_selection import GridSearchCV

param_grid = [
	{'n_estimators':[3, 10, 30], 'max_features':[2, 4, 6, 8]},
	{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
]

grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')
grid_search.fit(train_housing_data_prepared, train_housing_labels)
```

###### 5.2.2	Randomized Search

​	给定一个尝试的次数，该函数随机的选择参数并尝试多次。

###### 5.2.3	研究最好的模型，以及它们的缺点

​	假设我们仍然使用目前效果最好的随机森林模型。在上面我们对一些参数进行了测试，接着我们可以观测训练集中的特征值的重要性，看看哪些特征值对结果影响很大，而哪些特征值则影响比较小。

```python
feature_importances = grid_search.best_estimator_.feature_importances_
extra_attribs = ["rooms_per_hhold", "pop_per_hhold", "bedrooms_per_room"]
cat_one_hot_attribs = list(encoder.classes_)
attributes = num_attribs + extra_attribs + cat_one_hot_attribs
print(sorted(zip(feature_importances, attributes), reverse=True))
```

```python
[(0.32649798665134971, 'median_income'), 
 (0.15334491760305854, 'INLAND'), 
 (0.11305529021187399, 'pop_per_hhold'), 
 (0.07793247662544775, 'bedrooms_per_room'), 
 (0.071415642259275158, 'longitude'), 
 (0.067613918945568688, 'latitude'), 
 (0.060436577499703222, 'rooms_per_hhold'), 
 (0.04442608939578685, 'housing_median_age'), 
 (0.018240254462909437, 'population'), 
 (0.01663085833886218, 'total_rooms'), 
 (0.016607686091288865, 'total_bedrooms'),
 (0.016345876147580776, 'households'), 
 (0.011216644219017424, '<1H OCEAN'), 
 (0.0034668118081117387, 'NEAR OCEAN'), 
 (0.0026848388432755429, 'NEAR BAY'), 
 (8.4130896890070617e-05, 'ISLAND')]
```

​	有了上述数据，我们发现可以将一些无关紧要的数据丢弃掉，例如`ocean_proximity`类别中只有`INLAND`特别重要，其他的都没什么用，那么我们就可以其他的数据删掉。

#### 6	进行回归

​	接下来，就是使用测试集进行测试了。